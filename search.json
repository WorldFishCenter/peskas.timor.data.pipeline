[{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement [INSERT CONTACT METHOD]. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/CODE_OF_CONDUCT.html","id":"1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/CODE_OF_CONDUCT.html","id":"2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/CODE_OF_CONDUCT.html","id":"3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/CODE_OF_CONDUCT.html","id":"4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.0, available https://www.contributor-covenant.org/version/2/0/code_of_conduct.html. Community Impact Guidelines inspired Mozilla’s code conduct enforcement ladder. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https:// www.contributor-covenant.org/translations.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to peskas.timor.data.pipeline","title":"Contributing to peskas.timor.data.pipeline","text":"outlines propose change peskas.timor.data.pipeline. detailed info contributing , tidyverse packages, please see development contributing guide.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/CONTRIBUTING.html","id":"fixing-typos","dir":"","previous_headings":"","what":"Fixing typos","title":"Contributing to peskas.timor.data.pipeline","text":"can fix typos, spelling mistakes, grammatical errors documentation directly using GitHub web interface, long changes made source file. generally means ’ll need edit roxygen2 comments .R, .Rd file. can find .R file generates .Rd reading comment first line.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/CONTRIBUTING.html","id":"bigger-changes","dir":"","previous_headings":"","what":"Bigger changes","title":"Contributing to peskas.timor.data.pipeline","text":"want make bigger change, ’s good idea first file issue make sure someone team agrees ’s needed. ’ve found bug, please file issue illustrates bug minimal reprex (also help write unit test, needed).","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/CONTRIBUTING.html","id":"pull-request-process","dir":"","previous_headings":"Bigger changes","what":"Pull request process","title":"Contributing to peskas.timor.data.pipeline","text":"Fork package clone onto computer. haven’t done , recommend using usethis::create_from_github(\"WorldFishCenter/peskas.timor.data.pipeline\", fork = TRUE). Install development dependences devtools::install_dev_deps(), make sure package passes R CMD check running devtools::check(). R CMD check doesn’t pass cleanly, ’s good idea ask help continuing. Create Git branch pull request (PR). recommend using usethis::pr_init(\"brief-description--change\"). Make changes, commit git, create PR running usethis::pr_push(), following prompts browser. title PR briefly describe change. body PR contain Fixes #issue-number. user-facing changes, add bullet top NEWS.md (.e. just first header). Follow style described https://style.tidyverse.org/news.html.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/CONTRIBUTING.html","id":"code-style","dir":"","previous_headings":"Bigger changes","what":"Code style","title":"Contributing to peskas.timor.data.pipeline","text":"New code follow tidyverse style guide. can use styler package apply styles, please don’t restyle code nothing PR. use roxygen2, Markdown syntax, documentation. use testthat unit tests. Contributions test cases included easier accept.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/CONTRIBUTING.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Contributing to peskas.timor.data.pipeline","text":"Please note peskas.timor.data.pipeline project released Contributor Code Conduct. contributing project agree abide terms.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 3, 29 June 2007Copyright © 2007 Free Software Foundation, Inc. <http://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"GNU General Public License free, copyleft license software kinds works. licenses software practical works designed take away freedom share change works. contrast, GNU General Public License intended guarantee freedom share change versions program–make sure remains free software users. , Free Software Foundation, use GNU General Public License software; applies also work released way authors. can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge wish), receive source code can get want , can change software use pieces new free programs, know can things. protect rights, need prevent others denying rights asking surrender rights. Therefore, certain responsibilities distribute copies software, modify : responsibilities respect freedom others. example, distribute copies program, whether gratis fee, must pass recipients freedoms received. must make sure , , receive can get source code. must show terms know rights. Developers use GNU GPL protect rights two steps: (1) assert copyright software, (2) offer License giving legal permission copy, distribute /modify . developers’ authors’ protection, GPL clearly explains warranty free software. users’ authors’ sake, GPL requires modified versions marked changed, problems attributed erroneously authors previous versions. devices designed deny users access install run modified versions software inside , although manufacturer can . fundamentally incompatible aim protecting users’ freedom change software. systematic pattern abuse occurs area products individuals use, precisely unacceptable. Therefore, designed version GPL prohibit practice products. problems arise substantially domains, stand ready extend provision domains future versions GPL, needed protect freedom users. Finally, every program threatened constantly software patents. States allow patents restrict development use software general-purpose computers, , wish avoid special danger patents applied free program make effectively proprietary. prevent , GPL assures patents used render program non-free. precise terms conditions copying, distribution modification follow.","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/LICENSE.html","id":"0-definitions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"0. Definitions","title":"GNU General Public License","text":"“License” refers version 3 GNU General Public License. “Copyright” also means copyright-like laws apply kinds works, semiconductor masks. “Program” refers copyrightable work licensed License. licensee addressed “”. “Licensees” “recipients” may individuals organizations. “modify” work means copy adapt part work fashion requiring copyright permission, making exact copy. resulting work called “modified version” earlier work work “based ” earlier work. “covered work” means either unmodified Program work based Program. “propagate” work means anything , without permission, make directly secondarily liable infringement applicable copyright law, except executing computer modifying private copy. Propagation includes copying, distribution (without modification), making available public, countries activities well. “convey” work means kind propagation enables parties make receive copies. Mere interaction user computer network, transfer copy, conveying. interactive user interface displays “Appropriate Legal Notices” extent includes convenient prominently visible feature (1) displays appropriate copyright notice, (2) tells user warranty work (except extent warranties provided), licensees may convey work License, view copy License. interface presents list user commands options, menu, prominent item list meets criterion.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/LICENSE.html","id":"1-source-code","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"1. Source Code","title":"GNU General Public License","text":"“source code” work means preferred form work making modifications . “Object code” means non-source form work. “Standard Interface” means interface either official standard defined recognized standards body, , case interfaces specified particular programming language, one widely used among developers working language. “System Libraries” executable work include anything, work whole, () included normal form packaging Major Component, part Major Component, (b) serves enable use work Major Component, implement Standard Interface implementation available public source code form. “Major Component”, context, means major essential component (kernel, window system, ) specific operating system () executable work runs, compiler used produce work, object code interpreter used run . “Corresponding Source” work object code form means source code needed generate, install, (executable work) run object code modify work, including scripts control activities. However, include work’s System Libraries, general-purpose tools generally available free programs used unmodified performing activities part work. example, Corresponding Source includes interface definition files associated source files work, source code shared libraries dynamically linked subprograms work specifically designed require, intimate data communication control flow subprograms parts work. Corresponding Source need include anything users can regenerate automatically parts Corresponding Source. Corresponding Source work source code form work.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/LICENSE.html","id":"2-basic-permissions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"2. Basic Permissions","title":"GNU General Public License","text":"rights granted License granted term copyright Program, irrevocable provided stated conditions met. License explicitly affirms unlimited permission run unmodified Program. output running covered work covered License output, given content, constitutes covered work. License acknowledges rights fair use equivalent, provided copyright law. may make, run propagate covered works convey, without conditions long license otherwise remains force. may convey covered works others sole purpose make modifications exclusively , provide facilities running works, provided comply terms License conveying material control copyright. thus making running covered works must exclusively behalf, direction control, terms prohibit making copies copyrighted material outside relationship . Conveying circumstances permitted solely conditions stated . Sublicensing allowed; section 10 makes unnecessary.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/LICENSE.html","id":"3-protecting-users-legal-rights-from-anti-circumvention-law","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"3. Protecting Users’ Legal Rights From Anti-Circumvention Law","title":"GNU General Public License","text":"covered work shall deemed part effective technological measure applicable law fulfilling obligations article 11 WIPO copyright treaty adopted 20 December 1996, similar laws prohibiting restricting circumvention measures. convey covered work, waive legal power forbid circumvention technological measures extent circumvention effected exercising rights License respect covered work, disclaim intention limit operation modification work means enforcing, work’s users, third parties’ legal rights forbid circumvention technological measures.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/LICENSE.html","id":"4-conveying-verbatim-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"4. Conveying Verbatim Copies","title":"GNU General Public License","text":"may convey verbatim copies Program’s source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice; keep intact notices stating License non-permissive terms added accord section 7 apply code; keep intact notices absence warranty; give recipients copy License along Program. may charge price price copy convey, may offer support warranty protection fee.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/LICENSE.html","id":"5-conveying-modified-source-versions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"5. Conveying Modified Source Versions","title":"GNU General Public License","text":"may convey work based Program, modifications produce Program, form source code terms section 4, provided also meet conditions: ) work must carry prominent notices stating modified , giving relevant date. b) work must carry prominent notices stating released License conditions added section 7. requirement modifies requirement section 4 “keep intact notices”. c) must license entire work, whole, License anyone comes possession copy. License therefore apply, along applicable section 7 additional terms, whole work, parts, regardless packaged. License gives permission license work way, invalidate permission separately received . d) work interactive user interfaces, must display Appropriate Legal Notices; however, Program interactive interfaces display Appropriate Legal Notices, work need make . compilation covered work separate independent works, nature extensions covered work, combined form larger program, volume storage distribution medium, called “aggregate” compilation resulting copyright used limit access legal rights compilation’s users beyond individual works permit. Inclusion covered work aggregate cause License apply parts aggregate.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/LICENSE.html","id":"6-conveying-non-source-forms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"6. Conveying Non-Source Forms","title":"GNU General Public License","text":"may convey covered work object code form terms sections 4 5, provided also convey machine-readable Corresponding Source terms License, one ways: ) Convey object code , embodied , physical product (including physical distribution medium), accompanied Corresponding Source fixed durable physical medium customarily used software interchange. b) Convey object code , embodied , physical product (including physical distribution medium), accompanied written offer, valid least three years valid long offer spare parts customer support product model, give anyone possesses object code either (1) copy Corresponding Source software product covered License, durable physical medium customarily used software interchange, price reasonable cost physically performing conveying source, (2) access copy Corresponding Source network server charge. c) Convey individual copies object code copy written offer provide Corresponding Source. alternative allowed occasionally noncommercially, received object code offer, accord subsection 6b. d) Convey object code offering access designated place (gratis charge), offer equivalent access Corresponding Source way place charge. need require recipients copy Corresponding Source along object code. place copy object code network server, Corresponding Source may different server (operated third party) supports equivalent copying facilities, provided maintain clear directions next object code saying find Corresponding Source. Regardless server hosts Corresponding Source, remain obligated ensure available long needed satisfy requirements. e) Convey object code using peer--peer transmission, provided inform peers object code Corresponding Source work offered general public charge subsection 6d. separable portion object code, whose source code excluded Corresponding Source System Library, need included conveying object code work. “User Product” either (1) “consumer product”, means tangible personal property normally used personal, family, household purposes, (2) anything designed sold incorporation dwelling. determining whether product consumer product, doubtful cases shall resolved favor coverage. particular product received particular user, “normally used” refers typical common use class product, regardless status particular user way particular user actually uses, expects expected use, product. product consumer product regardless whether product substantial commercial, industrial non-consumer uses, unless uses represent significant mode use product. “Installation Information” User Product means methods, procedures, authorization keys, information required install execute modified versions covered work User Product modified version Corresponding Source. information must suffice ensure continued functioning modified object code case prevented interfered solely modification made. convey object code work section , , specifically use , User Product, conveying occurs part transaction right possession use User Product transferred recipient perpetuity fixed term (regardless transaction characterized), Corresponding Source conveyed section must accompanied Installation Information. requirement apply neither third party retains ability install modified object code User Product (example, work installed ROM). requirement provide Installation Information include requirement continue provide support service, warranty, updates work modified installed recipient, User Product modified installed. Access network may denied modification materially adversely affects operation network violates rules protocols communication across network. Corresponding Source conveyed, Installation Information provided, accord section must format publicly documented (implementation available public source code form), must require special password key unpacking, reading copying.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/LICENSE.html","id":"7-additional-terms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"7. Additional Terms","title":"GNU General Public License","text":"“Additional permissions” terms supplement terms License making exceptions one conditions. Additional permissions applicable entire Program shall treated though included License, extent valid applicable law. additional permissions apply part Program, part may used separately permissions, entire Program remains governed License without regard additional permissions. convey copy covered work, may option remove additional permissions copy, part . (Additional permissions may written require removal certain cases modify work.) may place additional permissions material, added covered work, can give appropriate copyright permission. Notwithstanding provision License, material add covered work, may (authorized copyright holders material) supplement terms License terms: ) Disclaiming warranty limiting liability differently terms sections 15 16 License; b) Requiring preservation specified reasonable legal notices author attributions material Appropriate Legal Notices displayed works containing ; c) Prohibiting misrepresentation origin material, requiring modified versions material marked reasonable ways different original version; d) Limiting use publicity purposes names licensors authors material; e) Declining grant rights trademark law use trade names, trademarks, service marks; f) Requiring indemnification licensors authors material anyone conveys material (modified versions ) contractual assumptions liability recipient, liability contractual assumptions directly impose licensors authors. non-permissive additional terms considered “restrictions” within meaning section 10. Program received , part , contains notice stating governed License along term restriction, may remove term. license document contains restriction permits relicensing conveying License, may add covered work material governed terms license document, provided restriction survive relicensing conveying. add terms covered work accord section, must place, relevant source files, statement additional terms apply files, notice indicating find applicable terms. Additional terms, permissive non-permissive, may stated form separately written license, stated exceptions; requirements apply either way.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/LICENSE.html","id":"8-termination","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"8. Termination","title":"GNU General Public License","text":"may propagate modify covered work except expressly provided License. attempt otherwise propagate modify void, automatically terminate rights License (including patent licenses granted third paragraph section 11). However, cease violation License, license particular copyright holder reinstated () provisionally, unless copyright holder explicitly finally terminates license, (b) permanently, copyright holder fails notify violation reasonable means prior 60 days cessation. Moreover, license particular copyright holder reinstated permanently copyright holder notifies violation reasonable means, first time received notice violation License (work) copyright holder, cure violation prior 30 days receipt notice. Termination rights section terminate licenses parties received copies rights License. rights terminated permanently reinstated, qualify receive new licenses material section 10.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/LICENSE.html","id":"9-acceptance-not-required-for-having-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"9. Acceptance Not Required for Having Copies","title":"GNU General Public License","text":"required accept License order receive run copy Program. Ancillary propagation covered work occurring solely consequence using peer--peer transmission receive copy likewise require acceptance. However, nothing License grants permission propagate modify covered work. actions infringe copyright accept License. Therefore, modifying propagating covered work, indicate acceptance License .","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/LICENSE.html","id":"10-automatic-licensing-of-downstream-recipients","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"10. Automatic Licensing of Downstream Recipients","title":"GNU General Public License","text":"time convey covered work, recipient automatically receives license original licensors, run, modify propagate work, subject License. responsible enforcing compliance third parties License. “entity transaction” transaction transferring control organization, substantially assets one, subdividing organization, merging organizations. propagation covered work results entity transaction, party transaction receives copy work also receives whatever licenses work party’s predecessor interest give previous paragraph, plus right possession Corresponding Source work predecessor interest, predecessor can get reasonable efforts. may impose restrictions exercise rights granted affirmed License. example, may impose license fee, royalty, charge exercise rights granted License, may initiate litigation (including cross-claim counterclaim lawsuit) alleging patent claim infringed making, using, selling, offering sale, importing Program portion .","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/LICENSE.html","id":"11-patents","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"11. Patents","title":"GNU General Public License","text":"“contributor” copyright holder authorizes use License Program work Program based. work thus licensed called contributor’s “contributor version”. contributor’s “essential patent claims” patent claims owned controlled contributor, whether already acquired hereafter acquired, infringed manner, permitted License, making, using, selling contributor version, include claims infringed consequence modification contributor version. purposes definition, “control” includes right grant patent sublicenses manner consistent requirements License. contributor grants non-exclusive, worldwide, royalty-free patent license contributor’s essential patent claims, make, use, sell, offer sale, import otherwise run, modify propagate contents contributor version. following three paragraphs, “patent license” express agreement commitment, however denominated, enforce patent (express permission practice patent covenant sue patent infringement). “grant” patent license party means make agreement commitment enforce patent party. convey covered work, knowingly relying patent license, Corresponding Source work available anyone copy, free charge terms License, publicly available network server readily accessible means, must either (1) cause Corresponding Source available, (2) arrange deprive benefit patent license particular work, (3) arrange, manner consistent requirements License, extend patent license downstream recipients. “Knowingly relying” means actual knowledge , patent license, conveying covered work country, recipient’s use covered work country, infringe one identifiable patents country reason believe valid. , pursuant connection single transaction arrangement, convey, propagate procuring conveyance , covered work, grant patent license parties receiving covered work authorizing use, propagate, modify convey specific copy covered work, patent license grant automatically extended recipients covered work works based . patent license “discriminatory” include within scope coverage, prohibits exercise , conditioned non-exercise one rights specifically granted License. may convey covered work party arrangement third party business distributing software, make payment third party based extent activity conveying work, third party grants, parties receive covered work , discriminatory patent license () connection copies covered work conveyed (copies made copies), (b) primarily connection specific products compilations contain covered work, unless entered arrangement, patent license granted, prior 28 March 2007. Nothing License shall construed excluding limiting implied license defenses infringement may otherwise available applicable patent law.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/LICENSE.html","id":"12-no-surrender-of-others-freedom","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"12. No Surrender of Others’ Freedom","title":"GNU General Public License","text":"conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. convey covered work satisfy simultaneously obligations License pertinent obligations, consequence may convey . example, agree terms obligate collect royalty conveying convey Program, way satisfy terms License refrain entirely conveying Program.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/LICENSE.html","id":"13-use-with-the-gnu-affero-general-public-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"13. Use with the GNU Affero General Public License","title":"GNU General Public License","text":"Notwithstanding provision License, permission link combine covered work work licensed version 3 GNU Affero General Public License single combined work, convey resulting work. terms License continue apply part covered work, special requirements GNU Affero General Public License, section 13, concerning interaction network apply combination .","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/LICENSE.html","id":"14-revised-versions-of-this-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"14. Revised Versions of this License","title":"GNU General Public License","text":"Free Software Foundation may publish revised /new versions GNU General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies certain numbered version GNU General Public License “later version” applies , option following terms conditions either numbered version later version published Free Software Foundation. Program specify version number GNU General Public License, may choose version ever published Free Software Foundation. Program specifies proxy can decide future versions GNU General Public License can used, proxy’s public statement acceptance version permanently authorizes choose version Program. Later license versions may give additional different permissions. However, additional obligations imposed author copyright holder result choosing follow later version.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/LICENSE.html","id":"15-disclaimer-of-warranty","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"15. Disclaimer of Warranty","title":"GNU General Public License","text":"WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM “” WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/LICENSE.html","id":"16-limitation-of-liability","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"16. Limitation of Liability","title":"GNU General Public License","text":"EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MODIFIES /CONVEYS PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/LICENSE.html","id":"17-interpretation-of-sections-15-and-16","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"17. Interpretation of Sections 15 and 16","title":"GNU General Public License","text":"disclaimer warranty limitation liability provided given local legal effect according terms, reviewing courts shall apply local law closely approximates absolute waiver civil liability connection Program, unless warranty assumption liability accompanies copy Program return fee. END TERMS CONDITIONS","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively state exclusion warranty; file least “copyright” line pointer full notice found. Also add information contact electronic paper mail. program terminal interaction, make output short notice like starts interactive mode: hypothetical commands show w show c show appropriate parts General Public License. course, program’s commands might different; GUI interface, use “box”. also get employer (work programmer) school, , sign “copyright disclaimer” program, necessary. information , apply follow GNU GPL, see <http://www.gnu.org/licenses/>. GNU General Public License permit incorporating program proprietary programs. program subroutine library, may consider useful permit linking proprietary applications library. want , use GNU Lesser General Public License instead License. first, please read <http://www.gnu.org/philosophy/--lgpl.html>.","code":"<one line to give the program's name and a brief idea of what it does.> Copyright (C) 2021 WorldFish  This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have received a copy of the GNU General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>. peskas.timor.data.pipeline Copyright (C) 2021 WorldFish This program comes with ABSOLUTELY NO WARRANTY; for details type 'show w'. This is free software, and you are welcome to redistribute it under certain conditions; type 'show c' for details."},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/SUPPORT.html","id":null,"dir":"","previous_headings":"","what":"Getting help with peskas.timor.data.pipeline","title":"Getting help with peskas.timor.data.pipeline","text":"Thanks using peskas.timor.data.pipeline! filing issue, places explore pieces put together make process smooth possible.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/SUPPORT.html","id":"make-a-reprex","dir":"","previous_headings":"","what":"Make a reprex","title":"Getting help with peskas.timor.data.pipeline","text":"Start making minimal reproducible example using reprex package. haven’t heard used reprex , ’re treat! Seriously, reprex make R-question-asking endeavors easier (pretty insane ROI five ten minutes ’ll take learn ’s ). additional reprex pointers, check Get help! section tidyverse site.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/SUPPORT.html","id":"where-to-ask","dir":"","previous_headings":"","what":"Where to ask?","title":"Getting help with peskas.timor.data.pipeline","text":"Armed reprex, next step figure ask. ’s question: start community.rstudio.com, /StackOverflow. people answer questions. ’s bug: ’re right place, file issue. ’re sure: let community help figure ! problem bug feature request, can easily return report . opening new issue, sure search issues pull requests make sure bug hasn’t reported /already fixed development version. default, search pre-populated :issue :open. can edit qualifiers (e.g. :pr, :closed) needed. example, ’d simply remove :open search issues repo, open closed.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/SUPPORT.html","id":"what-happens-next","dir":"","previous_headings":"","what":"What happens next?","title":"Getting help with peskas.timor.data.pipeline","text":"efficient possible, development tidyverse packages tends bursty, shouldn’t worry don’t get immediate response. Typically don’t look repo sufficient quantity issues accumulates, ’s burst intense activity focus efforts. makes development efficient avoids expensive context switching problems, cost taking longer get back . process makes good reprex particularly important might multiple months initial report start working . can’t reproduce bug, can’t fix !","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Fernando Cagua. Author, maintainer. Lorenzo Longobardi. Author. WorldFish. Copyright holder.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Cagua F, Longobardi L (2023). peskas.timor.data.pipeline: Functions Implement Timor Small Scale Fisheries Data Pipeline. R package version 3.1.0.","code":"@Manual{,   title = {peskas.timor.data.pipeline: Functions to Implement the Timor Small Scale Fisheries Data Pipeline},   author = {Fernando Cagua and Lorenzo Longobardi},   year = {2023},   note = {R package version 3.1.0}, }"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/index.html","id":"peskastimordatapipeline","dir":"","previous_headings":"","what":"Functions to Implement the Timor Small Scale Fisheries\n    Data Pipeline","title":"Functions to Implement the Timor Small Scale Fisheries\n    Data Pipeline","text":"goal peskas.timor.data.pipeline implement, deploy, execute data modelling pipelines underpin Peskas-East Timor, small-scale fisheries analytics East Timor.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/index.html","id":"the-pipeline-is-an-r-package","dir":"","previous_headings":"","what":"The pipeline is an R package","title":"Functions to Implement the Timor Small Scale Fisheries\n    Data Pipeline","text":"peskas.timor.data.pipeline structured R package makes easier write production-grade software. Specifically, structuring code R package allows us : better handle system package dependencies, forces us split code functions, makes easier document code, makes easier test code make heavy use tidyverse style conventions usethis package automate tasks project setup deployment. information rationale structuring pipeline package check Chapter 3 Engineering Production-Grade Shiny Apps. book focused Shiny applications rationale also applies data pipelines production-ready code general. best place learn package development probably R packages book Hadley Wickham Jenny Brian.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/index.html","id":"the-pipeline-runs-on-github-actions","dir":"","previous_headings":"","what":"The pipeline runs on Github Actions","title":"Functions to Implement the Timor Small Scale Fisheries\n    Data Pipeline","text":"step pipeline defined function package, functions deployed integrated using GitHub Actions. allow us take advantages best practices continous development integration (CD/CI) automatically link code execution. However, workflow functions work almost scripts don’t take parameters used side effects. job pipeline defined workflow file: .github/workflows/data-pipeline.yaml can seen figure . Note additional workflows exist test package multiple environments build documentation website.  figure illustrate jobs part pipeline workflow. Note implemented yet. Generally, artifacts produced job stored cloud storage container retrieved cloud storage next job pipeline. storing job’s artifacts versioned using function add_version(), generally includes timestamp commit sha. approach allow us trace artifact unique run pipeline. retrieving jobs can call cloud_object_name() obtain latest specific version artifact.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/index.html","id":"environment-parameters-are-specified-in-the-config-file","dir":"","previous_headings":"","what":"Environment parameters are specified in the config file","title":"Functions to Implement the Timor Small Scale Fisheries\n    Data Pipeline","text":"parameters determine pipeline run specified inst/conf.yml. file can accessed using system.file(\"conf.yml\",package=\"peskas.timor.data.pipeline\"). Using file, opposed , example, including code, allows us easily switch parameters depending environment. use config package read configuration file. use three different environments (see ). determine environment use, config package checks environment variable R_CONFIG_ACTIVE. Remote development environment (default): development environment “default” configuration. environment used code running cloud One characteristic environment uses cloud storage buckets differ real application uses. makes ideal test code pipeline ’s deployed production. environment designed run cloud, indicates API tokens authentication files read environment variables. works well code runs GitHub Actions, workflow instructions read authentication details GitHub secrets passes R environment variables. Local development environment: “local” environment similar default environment used development therefore uses resources ideal testing code. main difference authentication information read environment variables local files. Specifically authentication files live directory called auth never committed git. possible run Sys.setenv(R_CONFIG_ACTIVE=\"local\") R console ensure local environment activated next time conf.yml read. even easier alternative add key-value pair R_CONFIG_ACTIVE=local .Renviron file project directory. Production environment: production environment similar default environment ’s designed run cloud read authentication details cloud. differs uses cloud resources used exclusively production things tested . environment active R_CONFIG_ACTIVE=production; environment variable passed pipeline workflow file code executes “main” git branch.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/index.html","id":"we-use-docker-containers","dir":"","previous_headings":"","what":"We use docker containers","title":"Functions to Implement the Timor Small Scale Fisheries\n    Data Pipeline","text":"use docker containers make easier run develop code. Development: use main Dockerfile development. ’s based rocker/geospatial image spins RStudio server instance quite large number packages. start instance container can simply go project’s directory run docker-compose -d --build terminal console. Production: use Dockerfile.prod run code production. image based lightweight version R installs required packages. first job pipeline builds container steps use run code. allow us run code environment regardless cloud computing infrastructure runs .","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/index.html","id":"logging","dir":"","previous_headings":"","what":"Logging","title":"Functions to Implement the Timor Small Scale Fisheries\n    Data Pipeline","text":"use logger package log events production.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/add_version.html","id":null,"dir":"Reference","previous_headings":"","what":"Add timestamp and sha string to a file name — add_version","title":"Add timestamp and sha string to a file name — add_version","text":"alternative version data name using sha (unique identifier) code using generate process data time data generated processed. function adds information, version identifier, file name (character string)","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/add_version.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add timestamp and sha string to a file name — add_version","text":"","code":"add_version(filename, extension = \"\", sha_nchar = 7, sep = \"__\")"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/add_version.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add timestamp and sha string to a file name — add_version","text":"filename Path sans extension file version extension Extension file sha_nchar Number characters SHA use version identifier sep Characters separating version identifier file name","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/add_version.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add timestamp and sha string to a file name — add_version","text":"character string file name version identifier","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/add_version.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add timestamp and sha string to a file name — add_version","text":"SHA information retrieved using git2r::sha. code running context aware git repository (example code running inside container) function attempts get sha environment variable GITHUB_SHA. methods fail, sha versioning added.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/add_version.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add timestamp and sha string to a file name — add_version","text":"","code":"if (git2r::in_repository()) {   add_version(\"my_file\", \"csv\") } #> [1] \"my_file__20231212105905_8741b84__.csv\""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/air_get_records.html","id":null,"dir":"Reference","previous_headings":"","what":"Get records from an Airtable table — air_get_records","title":"Get records from an Airtable table — air_get_records","text":"Downloads Airtable table list records","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/air_get_records.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get records from an Airtable table — air_get_records","text":"","code":"air_get_records(   table,   base_id,   api_key = Sys.getenv(\"AIRTABLE_KEY\"),   query = list() )"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/air_get_records.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get records from an Airtable table — air_get_records","text":"table name table (string) base_id id Airtable base table located api_key API key Airtable. default looks AIRTABLE_KEY environment variable. query additional parameters airtable query like example fields, view, pageSize, maxRecords, sort, filterByFormula, cellFormat, timeZone, userLocale, etc... See airtable documentation details.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/air_get_records.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get records from an Airtable table — air_get_records","text":"list results field containing records","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/air_get_records.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get records from an Airtable table — air_get_records","text":"Airtable API returns one page records time. page contain pageSize (query parameter) records, 100 default. Internally, function automatically download records maxRecords reached.","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/air_get_records.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get records from an Airtable table — air_get_records","text":"","code":"if (FALSE) { # Get all records for the table boats air_get_records(table = \"boats\", base_id = \"appjEVaN8kBNXAWak\")  # Get records only in a specified view air_get_records(\"boats\", \"appjEVaN8kBNXAWak\",   query = list(view = \"Boats with wrong registration\") )  # Get only the first 5 records air_get_records(\"boats\", \"appjEVaN8kBNXAWak\", query = list(maxRecords = 5)) }"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/air_records_to_tibble.html","id":null,"dir":"Reference","previous_headings":"","what":"Format Airtable records to a data frame (tibble) — air_records_to_tibble","title":"Format Airtable records to a data frame (tibble) — air_records_to_tibble","text":"Transforms list records obtained airtable api (example using air_get_records) data frame.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/air_records_to_tibble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Format Airtable records to a data frame (tibble) — air_records_to_tibble","text":"","code":"air_records_to_tibble(records)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/air_records_to_tibble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Format Airtable records to a data frame (tibble) — air_records_to_tibble","text":"records List records airtable table","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/air_records_to_tibble.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Format Airtable records to a data frame (tibble) — air_records_to_tibble","text":"tibble","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/air_records_to_tibble.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Format Airtable records to a data frame (tibble) — air_records_to_tibble","text":"Fields one item stored vector fields accept multiple items stored nested list column","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/air_records_to_tibble.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Format Airtable records to a data frame (tibble) — air_records_to_tibble","text":"","code":"if (FALSE) { # Get all records for the table boats air_get_records(table = \"boats\", base_id = \"appjEVaN8kBNXAWak\") %>%   air_record_to_tibble() }"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/air_tibble_to_records.html","id":null,"dir":"Reference","previous_headings":"","what":"Format a data frame (tibble) as Airtable records — air_tibble_to_records","title":"Format a data frame (tibble) as Airtable records — air_tibble_to_records","text":"Gets data frame converts list can passed air_upload_records() uploaded Airtable. required Airtable API accept tabular data certain restrictions format body request","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/air_tibble_to_records.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Format a data frame (tibble) as Airtable records — air_tibble_to_records","text":"","code":"air_tibble_to_records(   this_tibble,   id_fields = NULL,   link_fields = NULL,   max_records = 10 )"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/air_tibble_to_records.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Format a data frame (tibble) as Airtable records — air_tibble_to_records","text":"this_tibble data frame fields uploaded Airtable id_fields Character vector name column storing id link_fields Character vector name columns link fields (fields contain id another row's table) max_records Records grouped batches max_records. Defaults 10 current limit imposed Airtable API.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/air_tibble_to_records.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Format a data frame (tibble) as Airtable records — air_tibble_to_records","text":"list records","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/air_upload_records.html","id":null,"dir":"Reference","previous_headings":"","what":"Upload and create Airtable records to a base — air_upload_records","title":"Upload and create Airtable records to a base — air_upload_records","text":"Takes list created tibble_to_air_records() uploads Airtable. request_type set \"update\" body list must id field record. element body must 10 records long.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/air_upload_records.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upload and create Airtable records to a base — air_upload_records","text":"","code":"air_upload_records(   body,   table,   base_id,   api_key = Sys.getenv(\"AIRTABLE_KEY\"),   request_type = c(\"create\", \"update\") )"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/air_upload_records.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upload and create Airtable records to a base — air_upload_records","text":"body list records, formatted tibble_to_air_records() table name table (string) base_id id Airtable base table located api_key API key Airtable. default looks AIRTABLE_KEY environment variable. request_type Whether create new record update existing one. updating existing one, id_field needs specified calling tibble_to_air_records()","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/air_upload_records.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Upload and create Airtable records to a base — air_upload_records","text":"List request responses","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/alert_outlier.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate an alert vector based on the univOutl::LocScaleB() function — alert_outlier","title":"Generate an alert vector based on the univOutl::LocScaleB() function — alert_outlier","text":"Generate alert vector based univOutl::LocScaleB() function","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/alert_outlier.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate an alert vector based on the univOutl::LocScaleB() function — alert_outlier","text":"","code":"alert_outlier(   x,   no_alert_value = NA_real_,   alert_if_larger = no_alert_value,   alert_if_smaller = no_alert_value,   ... )"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/alert_outlier.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate an alert vector based on the univOutl::LocScaleB() function — alert_outlier","text":"x numeric vector outliers checked no_alert_value value put output alert (x within bounds) alert_if_larger alert x bounds found univOutl::LocScaleB() alert_if_smaller alert x bounds found univOutl::LocScaleB() ... arguments univOutl::LocScaleB()","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/alert_outlier.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate an alert vector based on the univOutl::LocScaleB() function — alert_outlier","text":"vector lenght x","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/authenticate_google_drive.html","id":null,"dir":"Reference","previous_headings":"","what":"Authenticate to Google Drive — authenticate_google_drive","title":"Authenticate to Google Drive — authenticate_google_drive","text":"Authenticate Google Drive","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/authenticate_google_drive.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Authenticate to Google Drive — authenticate_google_drive","text":"","code":"authenticate_google_drive(options = list(method = \"service_account_key\"))"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/authenticate_google_drive.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Authenticate to Google Drive — authenticate_google_drive","text":"options list information authentication method. Either \"service_account_key\" authenticating app \"OAuth\" authenticating real person (example running code locally).","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/calculate_weights.html","id":null,"dir":"Reference","previous_headings":"","what":"Add weight of species to merged landings — calculate_weights","title":"Add weight of species to merged landings — calculate_weights","text":"Downloads merged landings calculates weight catch.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/calculate_weights.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add weight of species to merged landings — calculate_weights","text":"","code":"calculate_weights(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/calculate_weights.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add weight of species to merged landings — calculate_weights","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/calculate_weights.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add weight of species to merged landings — calculate_weights","text":"outputs. function used side effects","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/calculate_weights.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add weight of species to merged landings — calculate_weights","text":" file uploaded cloud. name merged landings \"_weight\" end. parameters needed :   Progress function tracked using package logger.","code":"surveys:   merged_landings:     file_prefix:     version: storage:   storage_name:     key:     options:       project:       bucket:       service_account_key:"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/clean_catches.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean SFF landings' catches table — clean_catches","title":"Clean SFF landings' catches table — clean_catches","text":"Legacy data (SFF landings) include catches information different versions project. implies information (variables) can show different syntax /different column names. function converts recodes information associated species catches format adopted recent landings.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/clean_catches.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean SFF landings' catches table — clean_catches","text":"","code":"clean_catches(x)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/clean_catches.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean SFF landings' catches table — clean_catches","text":"x data frame containing raw legacy landings (SFF landings).","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/clean_catches.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean SFF landings' catches table — clean_catches","text":"data frame containing species catches information syntax recent landings.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/clean_catches.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean SFF landings' catches table — clean_catches","text":"","code":"if (FALSE) { authentication_details <- readLines(\"location_of_json_file.json\") # obtain the latest version of all files corresponding to timor-landings-v1 legacy_data <-   cloud_object_name(     prefix = \"timor-landings-v1\",     version = \"latest\",     provider = \"gcs\",     options = list(       service_account_key = authentication_details,       bucket = \"my-bucket\"     )   )  legacy_raw <-   readr::read_csv(file = legacy_data, col_types = readr::cols(.default = readr::col_character()))  clean_catches(legacy_raw) }"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/clean_legacy_landings.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean SFF landings data — clean_legacy_landings","title":"Clean SFF landings data — clean_legacy_landings","text":"Legacy landings data frame different structure uses different syntax recent landings. function restructures legacy raw data format syntax recent landings.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/clean_legacy_landings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean SFF landings data — clean_legacy_landings","text":"","code":"clean_legacy_landings(x)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/clean_legacy_landings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean SFF landings data — clean_legacy_landings","text":"x Data frame containing raw data legacy landings (SSF landings).","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/clean_legacy_landings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean SFF landings data — clean_legacy_landings","text":"Data frame containing legacy landings data syntax recent landings.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/clean_legacy_landings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean SFF landings data — clean_legacy_landings","text":"","code":"if (FALSE) { authentication_details <- readLines(\"location_of_json_file.json\") # obtain the latest version of all files corresponding to timor-landings-v1 legacy_data <-   cloud_object_name(     prefix = \"timor-landings-v1\",     version = \"latest\",     provider = \"gcs\",     options = list(       service_account_key = authentication_details,       bucket = \"my-bucket\"     )   )  legacy_raw <-   readr::read_csv(file = legacy_data, col_types = readr::cols(.default = readr::col_character()))  clean_legacy_landings(legacy_raw) }"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/clean_updated_landings.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean Updated (Peskas 2) landings data — clean_updated_landings","title":"Clean Updated (Peskas 2) landings data — clean_updated_landings","text":"New Peskas 2 survey landings include new questions new edits. function facilitates integration new survey data legacy ones.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/clean_updated_landings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean Updated (Peskas 2) landings data — clean_updated_landings","text":"","code":"clean_updated_landings(x)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/clean_updated_landings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean Updated (Peskas 2) landings data — clean_updated_landings","text":"x Data frame containing raw data updated landings (Peskas 2).","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/clean_updated_landings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean Updated (Peskas 2) landings data — clean_updated_landings","text":"Data frame containing updated landings data syntax old landings.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/clean_updated_landings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean Updated (Peskas 2) landings data — clean_updated_landings","text":"","code":"if (FALSE) { authentication_details <- readLines(\"location_of_json_file.json\") # obtain the latest version of all files corresponding to timor-landings-v1 peskas2_data <-   cloud_object_name(     prefix = \"timor-landings-v3\",     version = \"latest\",     provider = \"gcs\",     options = list(       service_account_key = authentication_details,       bucket = \"my-bucket\"     )   )  peskas2_raw <-   readr::read_csv(file = peskas2_data, col_types = readr::cols(.default = readr::col_character()))  clean_updated_landings(legacy_raw) }"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/cloud_object_name.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the full name of a versioned cloud object — cloud_object_name","title":"Get the full name of a versioned cloud object — cloud_object_name","text":"Obtain full name (e.g. timor-landings-v2_metadata__20210326084600_54617b3__.json) cloud storage object. one object matching prefix, version, extension, vector names returned.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/cloud_object_name.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the full name of a versioned cloud object — cloud_object_name","text":"","code":"cloud_object_name(   prefix,   version = \"latest\",   extension = \"\",   provider,   exact_match = FALSE,   options )"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/cloud_object_name.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the full name of a versioned cloud object — cloud_object_name","text":"prefix string indicating prefix object version either \"latest\" specific version string generated add_version file uploaded cloud provider extension extension desired file. Use empty string \"\" return extensions founds provider cloud provider use, either \"gcs\" \"aws\" exact_match logical indicating whether prefix matched exactly options named list cloud provider options, see details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/cloud_object_name.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the full name of a versioned cloud object — cloud_object_name","text":"string vector object names cloud storage match prefix, version, extension indicated parameters","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/cloud_object_name.html","id":"google-cloud-services","dir":"Reference","previous_headings":"","what":"Google Cloud Services","title":"Get the full name of a versioned cloud object — cloud_object_name","text":"Google Cloud Services (\"gcs\") options must list two fields: bucket bucketname (character) uploading , service_account_key contents authentication json file downloaded Google Project (cloud_storage_authenticate called ). function uses googleCloudStorageR::gcs_upload hood upload file.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/cloud_object_name.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get the full name of a versioned cloud object — cloud_object_name","text":"","code":"#' # Google Cloud Services if (FALSE) { authentication_details <- readLines(\"location_of_json_file.json\") # obtain the latest version of all files corresponding to timor-landings-v2 cloud_object_name(   prefix = \"timor-landings-v2\",   version = \"latest\",   provider = \"gcs\",   options = list(     service_account_key = authentication_details,     bucket = \"my-bucket\"   ) )  # obtain a specific version of the structured data from timor-landings-v2 cloud_object_name(   prefix = \"timor-landings-v2_raw\",   version = \"20210326084600_54617b\",   extension = \"csv\",   provider = \"gcs\",   options = list(     service_account_key = authentication_details,     bucket = \"my-bucket\"   ) ) }"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/cloud_storage_authenticate.html","id":null,"dir":"Reference","previous_headings":"","what":"Authenticate to a storage cloud provider — cloud_storage_authenticate","title":"Authenticate to a storage cloud provider — cloud_storage_authenticate","text":"Usually used internally functions","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/cloud_storage_authenticate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Authenticate to a storage cloud provider — cloud_storage_authenticate","text":"","code":"cloud_storage_authenticate(provider, options)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/cloud_storage_authenticate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Authenticate to a storage cloud provider — cloud_storage_authenticate","text":"provider cloud provider use, either \"gcs\" \"aws\" options named list cloud provider options, see details","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/cloud_storage_authenticate.html","id":"google-cloud-services","dir":"Reference","previous_headings":"","what":"Google Cloud Services","title":"Authenticate to a storage cloud provider — cloud_storage_authenticate","text":"Google Cloud Services (\"gcs\") options must list field service_account_key contents authentication json file downloaded Google Project. function uses googleCloudStorageR::gcs_auth hood authenticate.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/cloud_storage_authenticate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Authenticate to a storage cloud provider — cloud_storage_authenticate","text":"","code":"# Google Cloud Services if (FALSE) { authentication_details <- readLines(\"location_of_json_file.json\") cloud_storage_authenticate(   provider = \"gcs\",   options = list(     service_account_key = authentication_details,     bucket = \"my-bucket\"   ) ) }"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/coalist.html","id":null,"dir":"Reference","previous_headings":"","what":"Coalesce vectors — coalist","title":"Coalesce vectors — coalist","text":"function coalesces vectors (see dplyr::coalesce) selected data frame new single vector. vectors coalesce collected list (argument to_coal) output vector takes name list' element collected.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/coalist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coalesce vectors — coalist","text":"","code":"coalist(data, to_coal, return_dat = FALSE)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/coalist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Coalesce vectors — coalist","text":"data Data frame select vectors coalesce. to_coal List vectors coalesce. Coalesced vectors assume name element list collected. return_dat Logical argument indicating whether return whole data frame plus new coalesced vectors, new data frame constituted coalesced vectors.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/coalist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Coalesce vectors — coalist","text":"data frame containing coalesced vector(s)","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/coalist.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Coalesce vectors — coalist","text":"","code":"if (FALSE) { authentication_details <- readLines(\"location_of_json_file.json\") # obtain the latest version of all files corresponding to timor-landings-v1 legacy_data <-   cloud_object_name(     prefix = \"timor-landings-v1\",     version = \"latest\",     provider = \"gcs\",     options = list(       service_account_key = authentication_details,       bucket = \"my-bucket\"     )   )  legacy_raw <-   readr::read_csv(     file = legacy_data, col_types =       readr::cols(.default = readr::col_character())   )  to_coal <- list(   \"date\" = legacy_raw %>% dplyr::select(\"Data\", \"Date\"),   \"trip_group/gear_type\" = legacy_raw %>%     dplyr::select(tidyselect::contains(\"gear\")),   \"trip_group/habitat_boat\" = legacy_raw %>%     dplyr::select(tidyselect::contains(\"habitat\")) )  coalist(legacy_raw, to_coal, return_dat = FALSE) }"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/convert_taxa_names.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert taxa codes to common names — convert_taxa_names","title":"Convert taxa codes to common names — convert_taxa_names","text":"Convert taxa codes common names","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/convert_taxa_names.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert taxa codes to common names — convert_taxa_names","text":"","code":"convert_taxa_names(data, pars)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/convert_taxa_names.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert taxa codes to common names — convert_taxa_names","text":"data dataframe taxa codes column named \"catch_taxon\" pars config file","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/convert_taxa_names.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert taxa codes to common names — convert_taxa_names","text":"dataframe taxa common names","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/delete_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a dataset of a dataverse collection — delete_dataset","title":"Delete a dataset of a dataverse collection — delete_dataset","text":"function delete specific draft dataset.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/delete_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a dataset of a dataverse collection — delete_dataset","text":"","code":"delete_dataset(key, id, server)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/delete_dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a dataset of a dataverse collection — delete_dataset","text":"key API token associated Dataverse account. id dataset ID. server character string specifying Dataverse server.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/delete_dataverse.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a dataverse collection — delete_dataverse","title":"Delete a dataverse collection — delete_dataverse","text":"function delete specific Dataverse repository.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/delete_dataverse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a dataverse collection — delete_dataverse","text":"","code":"delete_dataverse(key, dataverse, server)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/delete_dataverse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a dataverse collection — delete_dataverse","text":"key API token associated Dataverse account. dataverse character string specifying Dataverse ID. server character string specifying Dataverse server.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/download_cloud_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Download an object from a cloud storage bucket to a local file — download_cloud_file","title":"Download an object from a cloud storage bucket to a local file — download_cloud_file","text":"Download object cloud storage local file","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/download_cloud_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download an object from a cloud storage bucket to a local file — download_cloud_file","text":"","code":"download_cloud_file(name, provider, options, file = name)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/download_cloud_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download an object from a cloud storage bucket to a local file — download_cloud_file","text":"name name object storage bucket. provider cloud provider use, either \"gcs\" \"aws\" options named list cloud provider options, see details file file-path (character) object saved. Default object name.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/download_cloud_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download an object from a cloud storage bucket to a local file — download_cloud_file","text":"file path","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/download_cloud_file.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download an object from a cloud storage bucket to a local file — download_cloud_file","text":"","code":"# Google Cloud Services if (FALSE) { authentication_details <- readLines(\"location_of_json_file.json\") download_cloud_file(   name = \"timor-landings-v2_metadata__20210326084600_54617b3__.json\",   provider = \"gcs\",   options = list(     service_account_key = authentication_details,     bucket = \"my-bucket\"   ) ) }"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/estimate_fishery_indicators.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate fisheries indicators — estimate_fishery_indicators","title":"Estimate fisheries indicators — estimate_fishery_indicators","text":"Uses trip data estimate various fisheries statistics.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/estimate_fishery_indicators.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate fisheries indicators — estimate_fishery_indicators","text":"","code":"estimate_fishery_indicators(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/estimate_fishery_indicators.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate fisheries indicators — estimate_fishery_indicators","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/estimate_fishery_indicators.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Estimate fisheries indicators — estimate_fishery_indicators","text":"parameters needed conf.yml :","code":"models:   file_prefix: storage:   storage_name:     key:     options:       project:       bucket:       service_account_key:"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/fill_missing_regions.html","id":null,"dir":"Reference","previous_headings":"","what":"Fill missing regions — fill_missing_regions","title":"Fill missing regions — fill_missing_regions","text":"Replace empty region data based boats geographic activity (trough tracker imeis)","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/fill_missing_regions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fill missing regions — fill_missing_regions","text":"","code":"fill_missing_regions(trips = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/fill_missing_regions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fill missing regions — fill_missing_regions","text":"trips Dataframe  Timor validated trips.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/fill_missing_regions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fill missing regions — fill_missing_regions","text":"dataframe filled regions (possible).","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/format_public_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Format public data — format_public_data","title":"Format public data — format_public_data","text":"Format merged trips files fit distribution. Specifically, function produces trips catch table well tables containing aggregated data. files stored tsv files rds storage bucket public access.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/format_public_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Format public data — format_public_data","text":"","code":"format_public_data(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/format_public_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Format public data — format_public_data","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/format_public_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Format public data — format_public_data","text":"outputs. function used side effects","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/format_public_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Format public data — format_public_data","text":"#'parameters needed config file required merge_trips() addition :","code":"public_storage:  google:    key: gcs    options:      project:      bucket:      service_account_key: export:  file_prefix:"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/generate_description.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate data description — generate_description","title":"Generate data description — generate_description","text":"function generate description data useful README file.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/generate_description.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate data description — generate_description","text":"","code":"generate_description(...)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/generate_description.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate data description — generate_description","text":"... unused; backwards compatibility ","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/generate_description.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate data description — generate_description","text":"list description variable uploaded dataset.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/generate_metadata.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a list of metadata — generate_metadata","title":"Generate a list of metadata — generate_metadata","text":"function generate list metadata information append files upload Dataverse repository.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/generate_metadata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a list of metadata — generate_metadata","text":"","code":"generate_metadata(pars, temp_coverage = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/generate_metadata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a list of metadata — generate_metadata","text":"pars configuration file. temp_coverage Temporal coverage data upload.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/generate_metadata.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a list of metadata — generate_metadata","text":"list metadata information","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_catch_types.html","id":null,"dir":"Reference","previous_headings":"","what":"Download metadata catch types — get_catch_types","title":"Download metadata catch types — get_catch_types","text":"function downloads airtable tables containing catches metadata.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_catch_types.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download metadata catch types — get_catch_types","text":"","code":"get_catch_types(pars)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_catch_types.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download metadata catch types — get_catch_types","text":"pars configuration file","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_catch_types.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download metadata catch types — get_catch_types","text":"dataframe containing taxonomic info common names associated species groups.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_distance.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate distance between consecutive trips — get_distance","title":"Estimate distance between consecutive trips — get_distance","text":"function calculates geographical distance meters end start two consecutive trips.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_distance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate distance between consecutive trips — get_distance","text":"","code":"get_distance(x)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_distance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate distance between consecutive trips — get_distance","text":"x data frame containing trips coordinates.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_distance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimate distance between consecutive trips — get_distance","text":"vector distances meters","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_fao_composition.html","id":null,"dir":"Reference","previous_headings":"","what":"Get FAO Food Composition Data — get_fao_composition","title":"Get FAO Food Composition Data — get_fao_composition","text":"function retrieves processes food composition data FAO database. specifically focuses marine food items octopus, squids, cockles, shrimps, crabs, lobsters. data filtered, categorized various nutrients including protein, calcium, iron, zinc, selenium, vitamin , omega-3 fatty acids.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_fao_composition.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get FAO Food Composition Data — get_fao_composition","text":"","code":"get_fao_composition()"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_fao_composition.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get FAO Food Composition Data — get_fao_composition","text":"tibble values various nutrients category marine food items. columns include Protein_mu, Calcium_mu, Iron_mu, Zinc_mu, Selenium_mu, Vitamin_A_mu, Omega_3_mu.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_fao_composition.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get FAO Food Composition Data — get_fao_composition","text":"function first reads CSV file given URL using read_csv function readr package. defines specific codes various marine food groups: octopus, squids, cockles, shrimps, crabs, lobsters. FAO composition data filtered food items raw state. new interagency_code created categorization purposes. nutrients considered protein, calcium, iron, zinc, selenium, vitamin , omega-3 fatty acids.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_fao_composition.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get FAO Food Composition Data — get_fao_composition","text":"","code":"get_fao_composition() #> Rows: 515 Columns: 13 #> ── Column specification ──────────────────────────────────────────────────────── #> Delimiter: \",\" #> chr (4): integragency_code, food_name, habitat, food_state #> dbl (9): food_id, ISSCAAP, protein(g), calcium(mg), iron(mg), zinc(mg), sele... #>  #> ℹ Use `spec()` to retrieve the full column specification for this data. #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #> # A tibble: 40 × 8 #>    interagency_code Protein_mu Calcium_mu Iron_mu Zinc_mu Selenium_mu #>    <chr>                 <dbl>      <dbl>   <dbl>   <dbl>       <dbl> #>  1 PEZ                    17.7         96     1.9    2.28          31 #>  2 CRA                    17.6         65     0.5    4.43          45 #>  3 CRA                    18.1         46     0.4    4.27          37 #>  4 CRA                    15.5         52     0.6    4.59          53 #>  5 CRA                    18.5         83     2.5    2.8           35 #>  6 CRA                    17.2        153     1.2    3.04          57 #>  7 CRA                    15.9         92     0.7    3.54          37 #>  8 CRA                    19.9        120     0.4    2.45          57 #>  9 CRA                    14.7        190     1.8    3.25          55 #> 10 SLV                    18.1         66     1.2    2.25          63 #> # ℹ 30 more rows #> # ℹ 2 more variables: Vitamin_A_mu <dbl>, Omega_3_mu <dbl>"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_fish_length.html","id":null,"dir":"Reference","previous_headings":"","what":"Download species body-length parameters — get_fish_length","title":"Download species body-length parameters — get_fish_length","text":"function takes species, group species common names arguments returns data frame containing weigth-length parameters FishBase database.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_fish_length.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download species body-length parameters — get_fish_length","text":"","code":"get_fish_length(taxa, rank = NULL, country_code = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_fish_length.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download species body-length parameters — get_fish_length","text":"taxa name species/groups retrieve lengths parameters. rank taxonomical ranking, can Species, Genus, Family, comm_name. latter abbreviation  common name species/group. country_code code country associated FishBase database, default code include Timor-Leste Indonesia. Full list https://www.fishbase.se/country/ListOfCountryCodes.php.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_fish_length.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download species body-length parameters — get_fish_length","text":"data frame containing species length parameters","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_fish_length.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download species body-length parameters — get_fish_length","text":"","code":"if (FALSE) { get_fish_length(   taxa = \"Sardine\", rank = \"comm_name\",   country_code = 626 ) }"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_full_tracks.html","id":null,"dir":"Reference","previous_headings":"","what":"Download tracks data in a single file — get_full_tracks","title":"Download tracks data in a single file — get_full_tracks","text":"Download latest version PDS single-file tracks data.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_full_tracks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download tracks data in a single file — get_full_tracks","text":"","code":"get_full_tracks(pars)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_full_tracks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download tracks data in a single file — get_full_tracks","text":"pars configuration file.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_full_tracks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download tracks data in a single file — get_full_tracks","text":"dataframes tracks coordinates trip","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_full_trips.html","id":null,"dir":"Reference","previous_headings":"","what":"Get trips from single-file tracks data. — get_full_trips","title":"Get trips from single-file tracks data. — get_full_trips","text":"Download list trips latest version PDS single-file tracks data.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_full_trips.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get trips from single-file tracks data. — get_full_trips","text":"","code":"get_full_trips(pars)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_full_trips.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get trips from single-file tracks data. — get_full_trips","text":"pars configuration file.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_full_trips.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get trips from single-file tracks data. — get_full_trips","text":"vector unique trips.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_host_url.html","id":null,"dir":"Reference","previous_headings":"","what":"Select server from kobo — get_host_url","title":"Select server from kobo — get_host_url","text":"Specifies Host URL API use","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_host_url.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select server from kobo — get_host_url","text":"","code":"get_host_url(api, version = \"v1\")"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_host_url.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select server from kobo — get_host_url","text":"api Either \"kobo\", \"kobohr\", \"ona\", custom (full) URL. API URLs made available KoBo Toolbox (\"kobo\", https://kc.kobotoolbox.org/api/v1/), KoBo Humanitarian Response (\"kobohr\", https://kc.humanitarianresponse.info/api/v1/), Ona (\"ona\", https://api.ona.io/api/v1/) Unhcr (\"unhcr\", https://kobocat.unhcr.org/api/v1/) . installation, installations using API accessed different URL, enter full URL. version Wether using API \"v1\" \"v2\"","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_host_url.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select server from kobo — get_host_url","text":"URL","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_host_url.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Select server from kobo — get_host_url","text":"Ananda Mahto","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_host_url.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Select server from kobo — get_host_url","text":"","code":"get_host_url(\"unhcr\") #> [1] \"https://kobocat.unhcr.org/api/v1\" get_host_url(\"https://kobocat.unhcr.org/api/v1/\") #> [1] \"https://kobocat.unhcr.org/api/v1/\""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_merged_landings.html","id":null,"dir":"Reference","previous_headings":"","what":"Download merged landings — get_merged_landings","title":"Download merged landings — get_merged_landings","text":"Download validated surveys landings PDS trips.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_merged_landings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download merged landings — get_merged_landings","text":"","code":"get_merged_landings(pars, suffix = \"\")"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_merged_landings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download merged landings — get_merged_landings","text":"pars Configuration file. suffix character indicating dataframe version. Use \"_weight\" download version calculated catch weight.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_merged_landings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download merged landings — get_merged_landings","text":"dataframe.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_merged_trips.html","id":null,"dir":"Reference","previous_headings":"","what":"Download Peskas surveys and PDS data — get_merged_trips","title":"Download Peskas surveys and PDS data — get_merged_trips","text":"Download validated Peskas surveys PDS data merged temporal matching","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_merged_trips.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download Peskas surveys and PDS data — get_merged_trips","text":"","code":"get_merged_trips(pars, ...)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_merged_trips.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download Peskas surveys and PDS data — get_merged_trips","text":"pars configuration file ... additional parameters","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_merged_trips.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download Peskas surveys and PDS data — get_merged_trips","text":"dataframe validated survey landings PDS trips.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_models.html","id":null,"dir":"Reference","previous_headings":"","what":"Download models estimates — get_models","title":"Download models estimates — get_models","text":"Download models estimates","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download models estimates — get_models","text":"","code":"get_models(pars)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download models estimates — get_models","text":"pars Configuration file","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download models estimates — get_models","text":"list dataframes national municipal estimations","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_nutrients_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Get taxa nutritional values — get_nutrients_table","title":"Get taxa nutritional values — get_nutrients_table","text":"Download modelled nutrional properties taxa according Hicks et al., 2019 (https://www.nature.com/articles/s41586-019-1592-6). table nutrients taxon directly called paper repository (https://github.com/mamacneil/NutrientFishbase).","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_nutrients_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get taxa nutritional values — get_nutrients_table","text":"","code":"get_nutrients_table(pars, summarise = TRUE, convert = TRUE)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_nutrients_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get taxa nutritional values — get_nutrients_table","text":"pars configuration file. summarise Whether want summarise nutritional values group. function use median default. convert Whether want convert nutritional values units grams. See details default starting units.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_nutrients_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get taxa nutritional values — get_nutrients_table","text":"data frame nutritional values taxa group.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_nutrients_table.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get taxa nutritional values — get_nutrients_table","text":"Starting units nutrient: mg/100g: Calcium, Iron Zinc. μg/100g: Selenium Vitamin . g/100g: Omega 3 Protein.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_preprocessed_metadata.html","id":null,"dir":"Reference","previous_headings":"","what":"Download Peskas metadata — get_preprocessed_metadata","title":"Download Peskas metadata — get_preprocessed_metadata","text":"Download preprocessed Peskas metadata Google Cloud","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_preprocessed_metadata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download Peskas metadata — get_preprocessed_metadata","text":"","code":"get_preprocessed_metadata(pars)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_preprocessed_metadata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download Peskas metadata — get_preprocessed_metadata","text":"pars configuration file","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_sync_tracks.html","id":null,"dir":"Reference","previous_headings":"","what":"Download and synchronize tracks data in a single file — get_sync_tracks","title":"Download and synchronize tracks data in a single file — get_sync_tracks","text":"function downloads, synchronize ingest tracks data cloud storage single file. Since transferred data relatively large, synchronization uploading refreshed complete track file outdated least 5000 trips.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_sync_tracks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download and synchronize tracks data in a single file — get_sync_tracks","text":"","code":"get_sync_tracks(pars)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_sync_tracks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download and synchronize tracks data in a single file — get_sync_tracks","text":"pars configuration file.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_sync_tracks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download and synchronize tracks data in a single file — get_sync_tracks","text":"tabulated tracks collected Timor..","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_tracks_descriptors.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract summaries from pds tracks — get_tracks_descriptors","title":"Extract summaries from pds tracks — get_tracks_descriptors","text":"function extracts summaries tracks pds data useful validation. function extracts start_end_distance: distance initial final point trip, outliers_proportion: proportion track points exceeding 30 m/s trip, timetrace_dispersion: measure irregularity tracking signal.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_tracks_descriptors.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract summaries from pds tracks — get_tracks_descriptors","text":"","code":"get_tracks_descriptors(Trip, pars, tracks_list)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_tracks_descriptors.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract summaries from pds tracks — get_tracks_descriptors","text":"Trip vector pds trips process. pars configuration file. tracks_list list pds tracks files.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_tracks_descriptors.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract summaries from pds tracks — get_tracks_descriptors","text":"dataframe summaries pds trip ID.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_tracks_ids.html","id":null,"dir":"Reference","previous_headings":"","what":"Get pds IDs — get_tracks_ids","title":"Get pds IDs — get_tracks_ids","text":"Get list pds-tracks IDs stored pds bucket","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_tracks_ids.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get pds IDs — get_tracks_ids","text":"","code":"get_tracks_ids(pars)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_tracks_ids.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get pds IDs — get_tracks_ids","text":"pars configuration file.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_tracks_map.html","id":null,"dir":"Reference","previous_headings":"","what":"Get map of Timor pds tracks. — get_tracks_map","title":"Get map of Timor pds tracks. — get_tracks_map","text":"Download static map Timor pds tracks.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_tracks_map.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get map of Timor pds tracks. — get_tracks_map","text":"","code":"get_tracks_map(pars)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_tracks_map.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get map of Timor pds tracks. — get_tracks_map","text":"pars configuration file.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_validated_landings.html","id":null,"dir":"Reference","previous_headings":"","what":"Download Peskas validated landings — get_validated_landings","title":"Download Peskas validated landings — get_validated_landings","text":"Download validated Peskas data Google Cloud.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_validated_landings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download Peskas validated landings — get_validated_landings","text":"","code":"get_validated_landings(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_validated_landings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download Peskas validated landings — get_validated_landings","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_validated_landings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download Peskas validated landings — get_validated_landings","text":"dataframe validated survey landings.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_validation_sheet.html","id":null,"dir":"Reference","previous_headings":"","what":"Get peskas validation sheet — get_validation_sheet","title":"Get peskas validation sheet — get_validation_sheet","text":"Get peskas validation backup sheet google cloud","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_validation_sheet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get peskas validation sheet — get_validation_sheet","text":"","code":"get_validation_sheet(pars)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/get_validation_sheet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get peskas validation sheet — get_validation_sheet","text":"pars configuration file.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_complete_tracks.html","id":null,"dir":"Reference","previous_headings":"","what":"Ingest tracks data as a single file — ingest_complete_tracks","title":"Ingest tracks data as a single file — ingest_complete_tracks","text":"function uploads two files: data, complete tracks single rds file trips, vector containing unique trips data useful take track synchronization status data.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_complete_tracks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ingest tracks data as a single file — ingest_complete_tracks","text":"","code":"ingest_complete_tracks(pars, data = NULL, trips = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_complete_tracks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ingest tracks data as a single file — ingest_complete_tracks","text":"pars configuration file. data rds file containing tracks data. trips vector unique Trips argument data.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_complete_tracks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ingest tracks data as a single file — ingest_complete_tracks","text":"output. function used side effects","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_kepler_tracks.html","id":null,"dir":"Reference","previous_headings":"","what":"Ingest a Kepler.gl map — ingest_kepler_tracks","title":"Ingest a Kepler.gl map — ingest_kepler_tracks","text":"function use python library Kepler.gl https://docs.kepler.gl/docs/keplergl-jupyter generate upload map PDS tracks around Timor. uses reticulate::import_from_path load run python script kepler_mapper.py.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_kepler_tracks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ingest a Kepler.gl map — ingest_kepler_tracks","text":"","code":"ingest_kepler_tracks(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_kepler_tracks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ingest a Kepler.gl map — ingest_kepler_tracks","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_kepler_tracks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ingest a Kepler.gl map — ingest_kepler_tracks","text":"Nothing. function upload GCS.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_landings.html","id":null,"dir":"Reference","previous_headings":"","what":"Ingest Landings Survey data — ingest_landings","title":"Ingest Landings Survey data — ingest_landings","text":"Downloads landings information collected using Kobo Toolbox uploads cloud storage services.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_landings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ingest Landings Survey data — ingest_landings","text":"","code":"ingest_landings(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_landings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ingest Landings Survey data — ingest_landings","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_landings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ingest Landings Survey data — ingest_landings","text":"output. funcrion used side effects","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_landings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Ingest Landings Survey data — ingest_landings","text":"function downloads survey metadata (survey information) well survey responses. Afterwards uploads information cloud services. File names used contain versioning string includes date-time , available, first 7 digits git commit sha. acomplished using add_version() parameters needed conf.yml :   Progress function tracked using package logger.","code":"surveys:   landings_2:     api:     survey_id:     token:     file_prefix: storage:   storage_name:     key:     options:       project:       bucket:       service_account_key:"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_legacy_landings.html","id":null,"dir":"Reference","previous_headings":"","what":"Ingest legacy Landings Survey data — ingest_legacy_landings","title":"Ingest legacy Landings Survey data — ingest_legacy_landings","text":"Downloads legacy landings information collected using Kobo Toolbox uploads cloud storage services.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_legacy_landings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ingest legacy Landings Survey data — ingest_legacy_landings","text":"","code":"ingest_legacy_landings(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_legacy_landings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ingest legacy Landings Survey data — ingest_legacy_landings","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_legacy_landings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ingest legacy Landings Survey data — ingest_legacy_landings","text":"output. funcrion used side effects","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_legacy_landings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Ingest legacy Landings Survey data — ingest_legacy_landings","text":"function downloads survey metadata (survey information) well survey responses. Afterwards uploads information cloud services. File names used contain versioning string includes date-time , available, first 7 digits git commit sha. acomplished using add_version() parameters needed conf.yml :   Progress function tracked using package logger.","code":"surveys:   landings_1:     api:     survey_id:     token:     file_prefix: storage:   storage_name:     key:     options:       project:       bucket:       service_account_key:"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_metadata_tables.html","id":null,"dir":"Reference","previous_headings":"","what":"Ingest metadata tables — ingest_metadata_tables","title":"Ingest metadata tables — ingest_metadata_tables","text":"Metadata tables manually updated Airtable. function downloads table uploads drive location. tables include information boats, devices, municipalities, etc.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_metadata_tables.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ingest metadata tables — ingest_metadata_tables","text":"","code":"ingest_metadata_tables(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_metadata_tables.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ingest metadata tables — ingest_metadata_tables","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_metadata_tables.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Ingest metadata tables — ingest_metadata_tables","text":"parameters needed conf.yml :","code":"metadata:   airtable:     base_id:     name:     api_key:     tables:       -       - storage:   storage_name:     key:     options:       project:       bucket:       service_account_key:"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_pds_map.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate and ingest Timor maps — ingest_pds_map","title":"Generate and ingest Timor maps — ingest_pds_map","text":"function downloads pds tracks coordinates, generates png image showing map Timor divided municipalities including tracks paths, upload cloud storage. also upload data frame splitted grids produce leaflet maps portal.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_pds_map.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate and ingest Timor maps — ingest_pds_map","text":"","code":"ingest_pds_map(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_pds_map.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate and ingest Timor maps — ingest_pds_map","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_pds_map.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate and ingest Timor maps — ingest_pds_map","text":"output. function used side effects.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_pds_matched_trips.html","id":null,"dir":"Reference","previous_headings":"","what":"Ingest PDS matched trip — ingest_pds_matched_trips","title":"Ingest PDS matched trip — ingest_pds_matched_trips","text":"Ingest matched PDS tracks landing surveys compressed zip folder. GPS tracks file uploaded two files: one using original temporal resolution (1 sec recording frequency) one using lower temporal resolution based 1 minute scale.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_pds_matched_trips.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ingest PDS matched trip — ingest_pds_matched_trips","text":"","code":"ingest_pds_matched_trips(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_pds_matched_trips.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ingest PDS matched trip — ingest_pds_matched_trips","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_pds_matched_trips.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ingest PDS matched trip — ingest_pds_matched_trips","text":"outputs. function used side effects","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_pds_tracks.html","id":null,"dir":"Reference","previous_headings":"","what":"Ingest Pelagic Data System tracks data — ingest_pds_tracks","title":"Ingest Pelagic Data System tracks data — ingest_pds_tracks","text":"Downloads Pelagic Data System (pds) trips information uploads cloud storage services.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_pds_tracks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ingest Pelagic Data System tracks data — ingest_pds_tracks","text":"","code":"ingest_pds_tracks(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_pds_tracks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ingest Pelagic Data System tracks data — ingest_pds_tracks","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_pds_tracks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ingest Pelagic Data System tracks data — ingest_pds_tracks","text":"output. funcrion used side effects","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_pds_tracks.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Ingest Pelagic Data System tracks data — ingest_pds_tracks","text":"function  downloads uploads tracks data yet stored bucket. parameters needed conf.yml :   Progress function tracked using package logger.","code":"pds:   tracks:     token:     secret:     file_prefix: pds_storage:   storage_name:     key:     options:       project:       bucket:       service_account_key:"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_pds_trips.html","id":null,"dir":"Reference","previous_headings":"","what":"Ingest Pelagic Data System trips data — ingest_pds_trips","title":"Ingest Pelagic Data System trips data — ingest_pds_trips","text":"Downloads Pelagic Data System (pds) trips information uploads cloud storage services.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_pds_trips.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ingest Pelagic Data System trips data — ingest_pds_trips","text":"","code":"ingest_pds_trips(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_pds_trips.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ingest Pelagic Data System trips data — ingest_pds_trips","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_pds_trips.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ingest Pelagic Data System trips data — ingest_pds_trips","text":"output. funcrion used side effects","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_pds_trips.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Ingest Pelagic Data System trips data — ingest_pds_trips","text":"function downloads trips information Pelagic Data System devices. Afterwards uploads information cloud services. File names used contain versioning string includes date-time , available, first 7 digits git commit sha. acomplished using add_version() parameters needed conf.yml :   Progress function tracked using package logger.","code":"pds:   trips:     token:     secret:     file_prefix: pds_storage:   storage_name:     key:     options:       project:       bucket:       service_account_key:"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_rfish_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Ingest species weight-length info — ingest_rfish_table","title":"Ingest species weight-length info — ingest_rfish_table","text":"Ingest species weight-length info","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_rfish_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ingest species weight-length info — ingest_rfish_table","text":"","code":"ingest_rfish_table(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_rfish_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ingest species weight-length info — ingest_rfish_table","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_rfish_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ingest species weight-length info — ingest_rfish_table","text":"output. function used side effects","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_updated_landings.html","id":null,"dir":"Reference","previous_headings":"","what":"Ingest Landings Survey data (Peskas 2) — ingest_updated_landings","title":"Ingest Landings Survey data (Peskas 2) — ingest_updated_landings","text":"Downloads updated (Peskas 2) landings information collected using Kobo Toolbox uploads cloud storage services.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_updated_landings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ingest Landings Survey data (Peskas 2) — ingest_updated_landings","text":"","code":"ingest_updated_landings(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_updated_landings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ingest Landings Survey data (Peskas 2) — ingest_updated_landings","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_updated_landings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ingest Landings Survey data (Peskas 2) — ingest_updated_landings","text":"output. funcrion used side effects","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_updated_landings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Ingest Landings Survey data (Peskas 2) — ingest_updated_landings","text":"function downloads survey metadata (survey information) well survey responses. Afterwards uploads information cloud services. File names used contain versioning string includes date-time , available, first 7 digits git commit sha. acomplished using add_version() parameters needed conf.yml :   Progress function tracked using package logger.","code":"surveys:   landings_3:     api:     survey_id:     token:     file_prefix: storage:   storage_name:     key:     options:       project:       bucket:       service_account_key:"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_validation_tables.html","id":null,"dir":"Reference","previous_headings":"","what":"Ingest flags tables — ingest_validation_tables","title":"Ingest flags tables — ingest_validation_tables","text":"Flags tables manually updated Google Drive spreadsheet. function downloads table uploads drive location. tables include information flags, boats, devices, municipalities, etc.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_validation_tables.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ingest flags tables — ingest_validation_tables","text":"","code":"ingest_validation_tables(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_validation_tables.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ingest flags tables — ingest_validation_tables","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/ingest_validation_tables.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Ingest flags tables — ingest_validation_tables","text":"parameters needed conf.yml :","code":"flags:   airtable:     base_id:     name:     api_key:     tables:       -       - storage:   storage_name:     key:     options:       project:       bucket:       service_account_key:"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/insistent_download_cloud_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Insistent version of download_cloud_file() — insistent_download_cloud_file","title":"Insistent version of download_cloud_file() — insistent_download_cloud_file","text":"Just like download_cloud_file(), function takes vector tracks files argument download cloud. function uses purrr::insistently order continue download files despite stale OAuth token.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/insistent_download_cloud_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Insistent version of download_cloud_file() — insistent_download_cloud_file","text":"","code":"insistent_download_cloud_file(..., delay = 3)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/insistent_download_cloud_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Insistent version of download_cloud_file() — insistent_download_cloud_file","text":"... Inputs download_cloud_file() delay time interval suspend execution , seconds.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/insistent_download_cloud_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Insistent version of download_cloud_file() — insistent_download_cloud_file","text":"output. function used side effects","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/insistent_upload_cloud_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Insistent version of upload_cloud_file() — insistent_upload_cloud_file","title":"Insistent version of upload_cloud_file() — insistent_upload_cloud_file","text":"Just like upload_cloud_file(), function takes vector tracks files argument upload cloud. function uses purrr::insistently order continue upload files despite stale OAuth token.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/insistent_upload_cloud_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Insistent version of upload_cloud_file() — insistent_upload_cloud_file","text":"","code":"insistent_upload_cloud_file(..., delay = 3)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/insistent_upload_cloud_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Insistent version of upload_cloud_file() — insistent_upload_cloud_file","text":"... Inputs upload_cloud_file() delay time interval suspend execution , seconds.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/insistent_upload_cloud_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Insistent version of upload_cloud_file() — insistent_upload_cloud_file","text":"output. function used side effects","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/join_weights.html","id":null,"dir":"Reference","previous_headings":"","what":"Join length-weights and nutritional parameters info to preprocessed landings — join_weights","title":"Join length-weights and nutritional parameters info to preprocessed landings — join_weights","text":"function integrate nutritional info get_nutrients_table function length-weight info FIshBase databse preprocessed surveys data convert catch  labels according FAO nomenclature (http://www.fao.org/fishery/statistics/global-production/3/en).","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/join_weights.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Join length-weights and nutritional parameters info to preprocessed landings — join_weights","text":"","code":"join_weights(data, metadata, rfish_tab, nutrients_table)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/join_weights.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Join length-weights and nutritional parameters info to preprocessed landings — join_weights","text":"data survey landings data frame metadata Metadata tables rfish_tab Table length weight parameters nutrients_table Table nutritional parameters","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/join_weights.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Join length-weights and nutritional parameters info to preprocessed landings — join_weights","text":"new landings data frame including length-weights info","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/join_weights.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Join length-weights and nutritional parameters info to preprocessed landings — join_weights","text":"length types used calculate weight fish catches include total length (TL) survey version 1 fork length (FL) survey version 2 exception group SRX (Myliobatiformes), uses disk width (WD). Weights Non-fish groups calculated according carapace width (CW) crabs, mantel length (ML) Cephalopoda shell length (ShL) Bivalvia, carapace length (CL) lobsters.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/kepler_mapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a Kepler.gl map — kepler_mapper","title":"Generate a Kepler.gl map — kepler_mapper","text":"function R wrapper kepler_mapper.py, python script function aimed elaborate produce self-contained map (html) using Kepler.gl python library https://docs.kepler.gl/docs/keplergl-jupyter.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/kepler_mapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a Kepler.gl map — kepler_mapper","text":"","code":"kepler_mapper(data_path = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/kepler_mapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a Kepler.gl map — kepler_mapper","text":"data_path Data add map.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/kepler_mapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a Kepler.gl map — kepler_mapper","text":"self-contained map html.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/merge_consecutive_trips.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge short intervalled trips — merge_consecutive_trips","title":"Merge short intervalled trips — merge_consecutive_trips","text":"PDS data quite patchy, sometimes actual single trip recorded 2 different trips. function calculates temporal geographical distance two consecutive trips boat, merging consecutive trips start 180 minutes following within range 2 km.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/merge_consecutive_trips.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge short intervalled trips — merge_consecutive_trips","text":"","code":"merge_consecutive_trips(   x,   consecutive_time = NULL,   consecutive_distance = NULL )"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/merge_consecutive_trips.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge short intervalled trips — merge_consecutive_trips","text":"x data frame containing raw pds trips data. consecutive_time Time consecutive trips (hrs). consecutive_distance Distance consecutive trips (Km).","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/merge_consecutive_trips.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge short intervalled trips — merge_consecutive_trips","text":"dataframe pds trips including new column associated_to indicating consecutive trips merged.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/merge_landings.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge recent and legacy pre-processed landings — merge_landings","title":"Merge recent and legacy pre-processed landings — merge_landings","text":"Downloads pre-processed versions recent legacy landings cloud merge rds-format file using function dplyr::full_join.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/merge_landings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge recent and legacy pre-processed landings — merge_landings","text":"","code":"merge_landings(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/merge_landings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge recent and legacy pre-processed landings — merge_landings","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/merge_landings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge recent and legacy pre-processed landings — merge_landings","text":"outputs. function used side effects","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/merge_landings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Merge recent and legacy pre-processed landings — merge_landings","text":"merged file uploaded cloud. parameters needed :   Progress function tracked using package logger.","code":"surveys:   landings:     file_prefix:     version:       preprocess:   landings_legacy:     file_prefix:     version:       preprocess:   merged_landings:     file_prefix:     version: storage:   storage_name:     key:     options:       project:       bucket:       service_account_key:"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/merge_trips.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge trips — merge_trips","title":"Merge trips — merge_trips","text":"Merges validated landings validated pds trips one single data frame. Merging done single landing tracked trip per day. Days determined using end tracking trip.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/merge_trips.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge trips — merge_trips","text":"","code":"merge_trips()"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/merge_trips.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge trips — merge_trips","text":"outputs. function used side effects","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/merge_trips.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Merge trips — merge_trips","text":"parameters needed config file required validate_landings(), validate_pds_trips()combined, well merged_trips.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/model_indicators.html","id":null,"dir":"Reference","previous_headings":"","what":"Model fisheries indicators — model_indicators","title":"Model fisheries indicators — model_indicators","text":"Uses trip data model various fisheries statistics.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/model_indicators.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model fisheries indicators — model_indicators","text":"","code":"model_indicators(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/model_indicators.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model fisheries indicators — model_indicators","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/model_indicators.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Model fisheries indicators — model_indicators","text":"parameters needed conf.yml :","code":"models:   file_prefix: storage:   storage_name:     key:     options:       project:       bucket:       service_account_key:"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/model_landings.html","id":null,"dir":"Reference","previous_headings":"","what":"Model number of landings per unit — model_landings","title":"Model number of landings per unit — model_landings","text":"Model number landings per unit","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/model_landings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model number of landings per unit — model_landings","text":"","code":"model_landings(trips)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/model_landings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model number of landings per unit — model_landings","text":"trips data frame","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/model_landings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model number of landings per unit — model_landings","text":"glmmTMB model","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/model_value.html","id":null,"dir":"Reference","previous_headings":"","what":"Model landing value — model_value","title":"Model landing value — model_value","text":"Model landing value","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/model_value.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model landing value — model_value","text":"","code":"model_value(trips)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/model_value.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model landing value — model_value","text":"trips data frame","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/model_value.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model landing value — model_value","text":"glmmTMB model","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_landings_step_1.html","id":null,"dir":"Reference","previous_headings":"","what":"Pre-process Timor Landings Survey data (step 1) — preprocess_landings_step_1","title":"Pre-process Timor Landings Survey data (step 1) — preprocess_landings_step_1","text":"Downloads raw structured data cloud storage services pre-process binary format easier deal R.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_landings_step_1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pre-process Timor Landings Survey data (step 1) — preprocess_landings_step_1","text":"","code":"preprocess_landings_step_1(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_landings_step_1.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pre-process Timor Landings Survey data (step 1) — preprocess_landings_step_1","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_landings_step_1.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pre-process Timor Landings Survey data (step 1) — preprocess_landings_step_1","text":"outputs. function used side effects","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_landings_step_1.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Pre-process Timor Landings Survey data (step 1) — preprocess_landings_step_1","text":"order exceed CPU memory limits Docker containers, preprocessing raw landings data splitted two containers (two separate jobs GitHub actions), function process first half raw data, function preprocess_landings_step_2 process second half. function downloads landings data given version (specified config file conf.yml.parameters needed :   Progress function tracked using package logger.","code":"surveys:   landings:     api:     survey_id:     token:     file_prefix:   version:     preprocess: storage:   storage_name:     key:     options:       project:       bucket:       service_account_key:"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_landings_step_2.html","id":null,"dir":"Reference","previous_headings":"","what":"Pre-process Timor Landings Survey data (step 2) — preprocess_landings_step_2","title":"Pre-process Timor Landings Survey data (step 2) — preprocess_landings_step_2","text":"Downloads raw structured data cloud storage services pre-process binary format easier deal R.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_landings_step_2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pre-process Timor Landings Survey data (step 2) — preprocess_landings_step_2","text":"","code":"preprocess_landings_step_2(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_landings_step_2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pre-process Timor Landings Survey data (step 2) — preprocess_landings_step_2","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_landings_step_2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pre-process Timor Landings Survey data (step 2) — preprocess_landings_step_2","text":"outputs. function used side effects","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_landings_step_2.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Pre-process Timor Landings Survey data (step 2) — preprocess_landings_step_2","text":"order exceed CPU memory limits Docker containers, preprocessing raw landings data splitted two containers (two separate jobs GitHub actions), function process second half raw data, function preprocess_landings_step_1 process first half. function downloads landings data given version (specified config file conf.yml.parameters needed :   Progress function tracked using package logger.","code":"surveys:   landings:     api:     survey_id:     token:     file_prefix:   version:     preprocess: storage:   storage_name:     key:     options:       project:       bucket:       service_account_key:"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_legacy_landings.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess Timor legacy Landings Survey data — preprocess_legacy_landings","title":"Preprocess Timor legacy Landings Survey data — preprocess_legacy_landings","text":"Downloads raw structured legacy data cloud storage services pre-process binary format easier deal R.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_legacy_landings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess Timor legacy Landings Survey data — preprocess_legacy_landings","text":"","code":"preprocess_legacy_landings(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_legacy_landings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess Timor legacy Landings Survey data — preprocess_legacy_landings","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_legacy_landings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preprocess Timor legacy Landings Survey data — preprocess_legacy_landings","text":"outputs. funcrion used side effects","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_legacy_landings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Preprocess Timor legacy Landings Survey data — preprocess_legacy_landings","text":"function downloads landings data given version (specified config file conf.yml. parameters needed :   Progress function tracked using package logger.","code":"surveys:   landings_1:    api:    survey_id:    token:    file_prefix:  version:   preprocess:  storage:   storage_name:     key:     options:       project:       bucket:       service_account_key:"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_metadata_tables.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess metadata tables — preprocess_metadata_tables","title":"Preprocess metadata tables — preprocess_metadata_tables","text":"Takes ingested metadata tables, validates , processes artefacts (devices information, flags) used pipeline.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_metadata_tables.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess metadata tables — preprocess_metadata_tables","text":"","code":"preprocess_metadata_tables(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_metadata_tables.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess metadata tables — preprocess_metadata_tables","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_metadata_tables.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Preprocess metadata tables — preprocess_metadata_tables","text":"Specifically function: Validates flags table containing columns: flag_id, flag_category, flag_message Constructs devices table containing columns: imei function requires arguments retrieves parameters conf.yml. fields required :","code":"metadata:   spreadsheet:     name:   version:     preprocess: storage:   storage_name:     key:     options:       project:       bucket:       service_account_key:"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_pds_tracks.html","id":null,"dir":"Reference","previous_headings":"","what":"Pre-process Pelagic Data System tracks — preprocess_pds_tracks","title":"Pre-process Pelagic Data System tracks — preprocess_pds_tracks","text":"Downloads raw structured data cloud storage services pre-process binary format easier deal R.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_pds_tracks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pre-process Pelagic Data System tracks — preprocess_pds_tracks","text":"","code":"preprocess_pds_tracks(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_pds_tracks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pre-process Pelagic Data System tracks — preprocess_pds_tracks","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_pds_tracks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pre-process Pelagic Data System tracks — preprocess_pds_tracks","text":"outputs. function used side effects","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_pds_tracks.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Pre-process Pelagic Data System tracks — preprocess_pds_tracks","text":"preprocessed file consists dataframe containing PDS tracks' diagnostics useful following validation step. parameters needed :   Progress function tracked using package logger.","code":"pds:  tracks:   token: !expr Sys.getenv('PDS_TOKEN')   secret: !expr Sys.getenv('PDS_SECRET')   file_prefix: pds_storage:   google:    options:     bucket: pds-timor"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_pds_trips.html","id":null,"dir":"Reference","previous_headings":"","what":"Pre-process Pelagic Data System trips — preprocess_pds_trips","title":"Pre-process Pelagic Data System trips — preprocess_pds_trips","text":"Downloads raw structured data cloud storage services pre-process binary format easier deal R.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_pds_trips.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pre-process Pelagic Data System trips — preprocess_pds_trips","text":"","code":"preprocess_pds_trips(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_pds_trips.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pre-process Pelagic Data System trips — preprocess_pds_trips","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_pds_trips.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pre-process Pelagic Data System trips — preprocess_pds_trips","text":"outputs. function used side effects","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_pds_trips.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Pre-process Pelagic Data System trips — preprocess_pds_trips","text":"function downloads landings data given version (specified config file conf.yml. parameters needed :   Progress function tracked using package logger.","code":"pds:   trips:     token:     secret:     file_prefix:   version:     preprocess: storage:   storage_name:     key:     options:       project:       bucket:       service_account_key:"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_updated_landings.html","id":null,"dir":"Reference","previous_headings":"","what":"Pre-process Timor Updated Landings Survey — preprocess_updated_landings","title":"Pre-process Timor Updated Landings Survey — preprocess_updated_landings","text":"Downloads raw structured data cloud storage services pre-process binary format easier deal R.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_updated_landings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pre-process Timor Updated Landings Survey — preprocess_updated_landings","text":"","code":"preprocess_updated_landings(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_updated_landings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pre-process Timor Updated Landings Survey — preprocess_updated_landings","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_updated_landings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pre-process Timor Updated Landings Survey — preprocess_updated_landings","text":"outputs. function used side effects","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/preprocess_updated_landings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Pre-process Timor Updated Landings Survey — preprocess_updated_landings","text":"function downloads landings data given version (specified config file conf.yml.parameters needed :   Progress function tracked using package logger.","code":"surveys:   landings:     api:     survey_id:     token:     file_prefix:   version:     preprocess: storage:   storage_name:     key:     options:       project:       bucket:       service_account_key:"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_nest_attachments.html","id":null,"dir":"Reference","previous_headings":"","what":"Nest attachment columns — pt_nest_attachments","title":"Nest attachment columns — pt_nest_attachments","text":"Nests attachment columns obtained reading structured data kobo landings survey","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_nest_attachments.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Nest attachment columns — pt_nest_attachments","text":"","code":"pt_nest_attachments(x)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_nest_attachments.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Nest attachment columns — pt_nest_attachments","text":"x data frame containing raw landings data Timor operations.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_nest_attachments.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Nest attachment columns — pt_nest_attachments","text":"Landings data information multiple attachments nested single column (_attachments) column contains tibble every row. , attachment tibble many rows attachments.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_nest_attachments.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Nest attachment columns — pt_nest_attachments","text":"One disadvantages using structured survey data tables can become wide (many columns). happens question groups fields can recorded multiple times. example landings survey, species captured, 17 questions recorded. limit number species can recorded trip. , example survey records seven species hundred columns data corresponding species information. improve situation avoid using multiple tables use nested data frames (see tidyr::nest). nested data frames columns can lists can contain arbitrary information, like data frames, lists, vectors, models.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_nest_attachments.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Nest attachment columns — pt_nest_attachments","text":"","code":"dummy_landings <- tidyr::tibble(   `_id` = \"123\",   `_attachments.0.download_url` = \"http://url-1.com\",   `_attachments.0.id` = \"01\",   `_attachments.1.download_url` = \"http://url-2.com\",   `_attachments.1.id` = \"02\",   `_attachments.2.download_url` = NA,   `_attachments.2.id` = NA ) pt_nest_attachments(dummy_landings) #> # A tibble: 1 × 2 #>   `_id` `_attachments`   #>   <chr> <list>           #> 1 123   <tibble [2 × 2]>"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_nest_species.html","id":null,"dir":"Reference","previous_headings":"","what":"Nest species columns — pt_nest_species","title":"Nest species columns — pt_nest_species","text":"Nests species columns obtained reading structured data kobo landings survey","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_nest_species.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Nest species columns — pt_nest_species","text":"","code":"pt_nest_species(x)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_nest_species.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Nest species columns — pt_nest_species","text":"x data frame containing raw landings data Timor operations.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_nest_species.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Nest species columns — pt_nest_species","text":"Landings data information multiple species nested single column (species_group) column contains tibble every row. , species tibble many rows species. turn, data frames contained species_group contain column called length_individuals data frame number individuals per length stored","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_nest_species.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Nest species columns — pt_nest_species","text":"One disadvantages using structured survey data tables can become wide (many columns). happens question groups fields can recorded multiple times. example landings survey, species captured, 17 questions recorded. limit number species can recorded trip. , example survey records seven species hundred columns data corresponding species information. improve situation avoid using multiple tables use nested data frames (see tidyr::nest). nested data frames columns can lists can contain arbitrary information, like data frames, lists, vectors, models.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_nest_species.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Nest species columns — pt_nest_species","text":"","code":"dummy_landings <- tidyr::tibble(   `_id` = \"123\",   `species_group.0.species_group/species` = \"sp01\",   `species_group.0.species_group/food_or_sale` = \"food\",   `species_group.0.species_group/no_fish_by_length_group/no_individuals_10_15` = 10,   `species_group.0.species_group/no_fish_by_length_group/no_individuals_5_10` = 10,   `species_group.0.species_group/no_fish_by_length_group/fish_length_over60` = 80,   `species_group.0.species_group/no_fish_by_length_group/no_individuals_over60` = 1,   `species_group.1.species_group/species` = \"sp02\",   `species_group.1.species_group/food_or_sale` = \"sale\",   `species_group.1.species_group/no_fish_by_length_group/no_individuals_20_25` = 5,   `species_group.1.species_group/no_fish_by_length_group/no_individuals_5_10` = 20,   `species_group.1.species_group/no_fish_by_length_group/fish_length_over60` = 70,   `species_group.1.species_group/no_fish_by_length_group/no_individuals_over60` = 2 ) pt_nest_species(dummy_landings) #> # A tibble: 1 × 2 #>   `_id` species_group    #>   <chr> <list>           #> 1 123   <tibble [2 × 4]>"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_boats.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse and validate boats table — pt_validate_boats","title":"Parse and validate boats table — pt_validate_boats","text":"Convert date date-time columns checks recorded length boats valid","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_boats.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse and validate boats table — pt_validate_boats","text":"","code":"pt_validate_boats(boats_table)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_boats.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse and validate boats table — pt_validate_boats","text":"boats_table data frame boats info","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_boats.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse and validate boats table — pt_validate_boats","text":"tibble","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_catch_types.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse and validate catch types table — pt_validate_catch_types","title":"Parse and validate catch types table — pt_validate_catch_types","text":"Currently function perform anything placeholder","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_catch_types.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse and validate catch types table — pt_validate_catch_types","text":"","code":"pt_validate_catch_types(catch_type_table)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_catch_types.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse and validate catch types table — pt_validate_catch_types","text":"catch_type_table data frame catch type info","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_catch_types.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse and validate catch types table — pt_validate_catch_types","text":"tibble","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_centro_pescas.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse and validate centro de pescas table — pt_validate_centro_pescas","title":"Parse and validate centro de pescas table — pt_validate_centro_pescas","text":"Currently function perform anything placeholder","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_centro_pescas.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse and validate centro de pescas table — pt_validate_centro_pescas","text":"","code":"pt_validate_centro_pescas(centro_pescas_table)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_centro_pescas.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse and validate centro de pescas table — pt_validate_centro_pescas","text":"centro_pescas_table data frame info centro de pescas","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_centro_pescas.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse and validate centro de pescas table — pt_validate_centro_pescas","text":"tibble","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_devices.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse and validate devices table — pt_validate_devices","title":"Parse and validate devices table — pt_validate_devices","text":"Convert date date-time columns ensures device_imei stored character. Currently table performs validations.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_devices.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse and validate devices table — pt_validate_devices","text":"","code":"pt_validate_devices(devices_table)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_devices.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse and validate devices table — pt_validate_devices","text":"devices_table data frame devices","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_devices.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse and validate devices table — pt_validate_devices","text":"tibble","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_fao_catch.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse and validate fao catch table — pt_validate_fao_catch","title":"Parse and validate fao catch table — pt_validate_fao_catch","text":"Currently function perform anything placeholder","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_fao_catch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse and validate fao catch table — pt_validate_fao_catch","text":"","code":"pt_validate_fao_catch(fao_catch_table)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_fao_catch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse and validate fao catch table — pt_validate_fao_catch","text":"fao_catch_table data frame catch type info","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_fao_catch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse and validate fao catch table — pt_validate_fao_catch","text":"tibble","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_flags.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate flags from metadata tables — pt_validate_flags","title":"Validate flags from metadata tables — pt_validate_flags","text":"Checks flags metadata tables. Specifically check flags unique flags id","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_flags.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate flags from metadata tables — pt_validate_flags","text":"","code":"pt_validate_flags(flags_table)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_flags.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate flags from metadata tables — pt_validate_flags","text":"flags_table table containing flags. Must columns flag_id, flag_category, flag_message","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_flags.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate flags from metadata tables — pt_validate_flags","text":"data frame columns flag_id, flag_category, flag_message","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_morphometric_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse and validate morphometric table — pt_validate_morphometric_table","title":"Parse and validate morphometric table — pt_validate_morphometric_table","text":"Parse validate morphometric table","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_morphometric_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse and validate morphometric table — pt_validate_morphometric_table","text":"","code":"pt_validate_morphometric_table(morphometric_table)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_morphometric_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse and validate morphometric table — pt_validate_morphometric_table","text":"morphometric_table data frame morphometric info","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_morphometric_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse and validate morphometric table — pt_validate_morphometric_table","text":"tibble","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_vms_installs.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse and validate vms_installs table — pt_validate_vms_installs","title":"Parse and validate vms_installs table — pt_validate_vms_installs","text":"Convert date date-time columns checks date device installations prior date recorded damage devices installed single boat","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_vms_installs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse and validate vms_installs table — pt_validate_vms_installs","text":"","code":"pt_validate_vms_installs(vms_installs_table)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_vms_installs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse and validate vms_installs table — pt_validate_vms_installs","text":"vms_installs_table data frame vms movements","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/pt_validate_vms_installs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse and validate vms_installs table — pt_validate_vms_installs","text":"tibble vms_installs","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/publish_dataverse.html","id":null,"dir":"Reference","previous_headings":"","what":"Publish a Dataverse repository — publish_dataverse","title":"Publish a Dataverse repository — publish_dataverse","text":"function publish specific Dataverse repository.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/publish_dataverse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Publish a Dataverse repository — publish_dataverse","text":"","code":"publish_dataverse(key, dataverse, server)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/publish_dataverse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Publish a Dataverse repository — publish_dataverse","text":"key API token associated Dataverse account. dataverse character string specifying Dataverse ID. server character string specifying Dataverse server.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/publish_last_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Publish latest dataset created — publish_last_dataset","title":"Publish latest dataset created — publish_last_dataset","text":"function converts \"draft\" \"public\" latest dataset uploaded specific Dataverse repository.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/publish_last_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Publish latest dataset created — publish_last_dataset","text":"","code":"publish_last_dataset(key = NULL, dataverse = NULL, server = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/publish_last_dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Publish latest dataset created — publish_last_dataset","text":"key API token associated Dataverse account. dataverse character string specifying Dataverse ID. server character string specifying Dataverse server.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/read_config.html","id":null,"dir":"Reference","previous_headings":"","what":"Read configuration file — read_config","title":"Read configuration file — read_config","text":"Reads configuration file conf.yml adds logging lines. Wrapped convenience","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/read_config.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read configuration file — read_config","text":"","code":"read_config()"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/read_config.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read configuration file — read_config","text":"environment parameters","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_lengths.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve weight-length parameters — retrieve_lengths","title":"Retrieve weight-length parameters — retrieve_lengths","text":"function takes table returned get_catch_types returns detailed weight-length parameters species group.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_lengths.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve weight-length parameters — retrieve_lengths","text":"","code":"retrieve_lengths(data, country_code)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_lengths.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve weight-length parameters — retrieve_lengths","text":"data table returned get_catch_types country_code code country associated FishBase database, default code include Timor-Leste Indonesia. Full list https://www.fishbase.se/country/ListOfCountryCodes.php.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_lengths.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve weight-length parameters — retrieve_lengths","text":"data frame containing species length parameters","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_lengths.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve weight-length parameters — retrieve_lengths","text":"","code":"if (FALSE) { rank_tab <- get_catch_types(pars) retrieve_lengths(rank_tab) }"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_pds_tracks.html","id":null,"dir":"Reference","previous_headings":"","what":"Download pelagic data system tracks — retrieve_pds_tracks","title":"Download pelagic data system tracks — retrieve_pds_tracks","text":"Download pelagic data system tracks","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_pds_tracks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download pelagic data system tracks — retrieve_pds_tracks","text":"","code":"retrieve_pds_tracks(prefix, secret, token, id)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_pds_tracks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download pelagic data system tracks — retrieve_pds_tracks","text":"prefix Name used prefix file names downloaded. Can path. secret Access secret code account token Access token account id Trip unique id download tracks data","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_pds_tracks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download pelagic data system tracks — retrieve_pds_tracks","text":"character vector paths downloaded files","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_pds_tracks.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download pelagic data system tracks — retrieve_pds_tracks","text":"","code":"if (FALSE) { retrieve_pds_tracks(   prefix = \"my-tracks\", secret = \"abcXXXXXX\",   token = \"123XXXXXX\", id = \"2327722\" ) # To download in a different path dir.create(\"my-data-dir\") retrieve_pds_tracks(   prefix = \"my-data-dir/my-tracks\", secret = \"abcXXXXXX\",   token = \"123XXXXXX\", id = \"2327722\" ) }"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_pds_tracks_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Download pelagic data system tracks in as csv — retrieve_pds_tracks_data","title":"Download pelagic data system tracks in as csv — retrieve_pds_tracks_data","text":"Download pelagic data system tracks csv","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_pds_tracks_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download pelagic data system tracks in as csv — retrieve_pds_tracks_data","text":"","code":"retrieve_pds_tracks_data(   path,   secret = NULL,   token = NULL,   id = NULL,   overwrite = TRUE )"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_pds_tracks_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download pelagic data system tracks in as csv — retrieve_pds_tracks_data","text":"path String path file API request saved secret Access secret code account token Access token account id Trip unique id download tracks data overwrite overwrite existing path TRUE.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_pds_tracks_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download pelagic data system tracks in as csv — retrieve_pds_tracks_data","text":"file path","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_pds_tracks_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download pelagic data system tracks in as csv — retrieve_pds_tracks_data","text":"","code":"retrieve_pds_tracks_data(\"test.csv\") #> [1] \"test.csv\" file.remove(\"test.csv\") #> [1] TRUE"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_pds_trips.html","id":null,"dir":"Reference","previous_headings":"","what":"Download pelagic data system trips — retrieve_pds_trips","title":"Download pelagic data system trips — retrieve_pds_trips","text":"Download pelagic data system trips","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_pds_trips.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download pelagic data system trips — retrieve_pds_trips","text":"","code":"retrieve_pds_trips(   prefix,   secret = NULL,   token = NULL,   start_date = \"2018-07-01\",   end_date = Sys.Date(),   append_version = TRUE )"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_pds_trips.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download pelagic data system trips — retrieve_pds_trips","text":"prefix Name used prefix file names downloaded. Can path. secret Access secret code account token Access token account start_date Start date download trips information end_date Last date download trips information append_version whether append versioning information filename using add_version.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_pds_trips.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download pelagic data system trips — retrieve_pds_trips","text":"character vector paths downloaded files","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_pds_trips.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download pelagic data system trips — retrieve_pds_trips","text":"","code":"if (FALSE) { retrieve_pds_trips(   prefix = \"my-trips\", secret = \"abcXXXXXX\",   token = \"123XXXXXX\", start_date = \"2018-07-01\",   end_date = Sys.Date() ) # To download in a different path dir.create(\"my-data-dir\") retrieve_pds_trips(   prefix = \"my-data-dir/my-trips\", secret = \"abcXXXXXX\",   token = \"123XXXXXX\", start_date = \"2018-07-01\",   end_date = Sys.Date() ) }"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_pds_trips_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Download pelagic data system trips in as csv or json — retrieve_pds_trips_data","title":"Download pelagic data system trips in as csv or json — retrieve_pds_trips_data","text":"Download pelagic data system trips csv json","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_pds_trips_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download pelagic data system trips in as csv or json — retrieve_pds_trips_data","text":"","code":"retrieve_pds_trips_data(   path,   secret = NULL,   token = NULL,   start_date = NULL,   end_date = NULL,   overwrite = TRUE )"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_pds_trips_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download pelagic data system trips in as csv or json — retrieve_pds_trips_data","text":"path String path file API request saved secret Access secret code account token Access token account start_date Start date download trips information end_date Last date download trips information overwrite overwrite existing path TRUE.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_pds_trips_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download pelagic data system trips in as csv or json — retrieve_pds_trips_data","text":"file path","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_pds_trips_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download pelagic data system trips in as csv or json — retrieve_pds_trips_data","text":"","code":"retrieve_pds_trips_data(\"test.csv\") #> [1] \"test.csv\" file.remove(\"test.csv\") #> [1] TRUE"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_survey.html","id":null,"dir":"Reference","previous_headings":"","what":"Download survey — retrieve_survey","title":"Download survey — retrieve_survey","text":"Download multiple survey artefacts (data metadata) . Progress function tracked using package logger.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_survey.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download survey — retrieve_survey","text":"","code":"retrieve_survey(   prefix,   api,   id,   token,   format = c(\"csv\", \"json\"),   metadata = TRUE,   append_version = TRUE )"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_survey.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download survey — retrieve_survey","text":"prefix name used prefix file names downloaded. Can path. api Either \"kobo\", \"kobohr\", \"ona\", custom (full) URL. API URLs made available KoBo Toolbox (\"kobo\", https://kc.kobotoolbox.org/api/v1/), KoBo Humanitarian Response (\"kobohr\", https://kc.humanitarianresponse.info/api/v1/), Ona (\"ona\", https://api.ona.io/api/v1/) Unhcr (\"unhcr\", https://kobocat.unhcr.org/api/v1/) . installation, installations using API accessed different URL, enter full URL. id survey id. Usually 6 digit number. See support page example can obtained token access token account e.g. \"Token XXXXXXX\" format Either \"csv\" \"json\" metadata whether download metadata well data append_version whether append versioning information filename using add_version.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_survey.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download survey — retrieve_survey","text":"character vector paths downloaded files","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_survey.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download survey — retrieve_survey","text":"","code":"if (FALSE) { # It's only possible to donwload survey metadata with the account Token retrieve_survey(   prefix = \"my-survey\", api = \"kobohr\", id = 753491,   token = \"123XXXXXX\" ) # To download in a different path dir.create(\"my-data-dir\") retrieve_survey(   prefix = \"my-data-dir/my-survey\", api = \"kobohr\", id = 753491,   token = \"123XXXXXX\" ) }"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_survey_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Download kobo data in as json using the v1 API — retrieve_survey_data","title":"Download kobo data in as json using the v1 API — retrieve_survey_data","text":"Download kobo data json using v1 API","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_survey_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download kobo data in as json using the v1 API — retrieve_survey_data","text":"","code":"retrieve_survey_data(   path,   id = NULL,   token = NULL,   api = \"kobohr\",   overwrite = TRUE )"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_survey_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download kobo data in as json using the v1 API — retrieve_survey_data","text":"path string path file API request saved id survey id. Usually 6 digit number. See support page example can obtained token access token account e.g. \"Token XXXXXXX\" api Either \"kobo\", \"kobohr\", \"ona\", custom (full) URL. API URLs made available KoBo Toolbox (\"kobo\", https://kc.kobotoolbox.org/api/v1/), KoBo Humanitarian Response (\"kobohr\", https://kc.humanitarianresponse.info/api/v1/), Ona (\"ona\", https://api.ona.io/api/v1/) Unhcr (\"unhcr\", https://kobocat.unhcr.org/api/v1/) . installation, installations using API accessed different URL, enter full URL. overwrite overwrite existing path TRUE.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_survey_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download kobo data in as json using the v1 API — retrieve_survey_data","text":"file path","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_survey_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download kobo data in as json using the v1 API — retrieve_survey_data","text":"","code":"if (FALSE) { retrieve_survey_data(\"test.json\", id = 753491, token = \"XXX\") file.remove(\"test.json\") }"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_survey_metadata.html","id":null,"dir":"Reference","previous_headings":"","what":"Download survey metadata — retrieve_survey_metadata","title":"Download survey metadata — retrieve_survey_metadata","text":"Download survey metadata","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_survey_metadata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download survey metadata — retrieve_survey_metadata","text":"","code":"retrieve_survey_metadata(id = NULL, token = NULL, api = \"kobohr\")"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_survey_metadata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download survey metadata — retrieve_survey_metadata","text":"id survey id. Usually 6 digit number. See support page example can obtained token access token account e.g. \"Token XXXXXXX\" api Either \"kobo\", \"kobohr\", \"ona\", custom (full) URL. API URLs made available KoBo Toolbox (\"kobo\", https://kc.kobotoolbox.org/api/v1/), KoBo Humanitarian Response (\"kobohr\", https://kc.humanitarianresponse.info/api/v1/), Ona (\"ona\", https://api.ona.io/api/v1/) Unhcr (\"unhcr\", https://kobocat.unhcr.org/api/v1/) . installation, installations using API accessed different URL, enter full URL.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_survey_metadata.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download survey metadata — retrieve_survey_metadata","text":"list survey metadata","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/retrieve_survey_metadata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download survey metadata — retrieve_survey_metadata","text":"","code":"if (FALSE) { # It's only possible to donwload survey metadata with the account Token retrieve_survey_metadata(753491, token = \"123XXXXXX\") }"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/send_sites_report.html","id":null,"dir":"Reference","previous_headings":"","what":"Send sites report email — send_sites_report","title":"Send sites report email — send_sites_report","text":"function takes advantage package blastula send email containing summary report landig site.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/send_sites_report.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Send sites report email — send_sites_report","text":"","code":"send_sites_report(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/send_sites_report.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Send sites report email — send_sites_report","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/send_sites_report.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Send sites report email — send_sites_report","text":"Nothing, function useful side effects.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/send_validation_mail.html","id":null,"dir":"Reference","previous_headings":"","what":"Send validation summary email — send_validation_mail","title":"Send validation summary email — send_validation_mail","text":"function takes advantage package blastula send email containing summary latest submissions problems.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/send_validation_mail.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Send validation summary email — send_validation_mail","text":"","code":"send_validation_mail(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/send_validation_mail.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Send validation summary email — send_validation_mail","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/send_validation_mail.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Send validation summary email — send_validation_mail","text":"Nothing, function useful side effects.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/tidyeval.html","id":null,"dir":"Reference","previous_headings":"","what":"Tidy eval helpers — tidyeval","title":"Tidy eval helpers — tidyeval","text":"sym() creates symbol string syms() creates list symbols character vector. enquo() enquos() delay execution one several function arguments. enquo() returns single quoted expression, like blueprint delayed computation. enquos() returns list quoted expressions. expr() quotes new expression locally. mostly useful build new expressions around arguments captured enquo() enquos(): expr(mean(!!enquo(arg), na.rm = TRUE)). as_name() transforms quoted variable name string. Supplying something else quoted variable name error. unlike as_label() also returns single string supports kind R object input, including quoted function calls vectors. purpose summarise object single label. label often suitable default name. know quoted expression contains (instance expressions captured enquo() variable name, call function, unquoted constant), use as_label(). know quoted simple variable name, like enforce , use as_name(). learn tidy eval use tools, visit https://tidyeval.tidyverse.org Metaprogramming section Advanced R.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/upload_cloud_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Upload a local file to a cloud storage bucket — upload_cloud_file","title":"Upload a local file to a cloud storage bucket — upload_cloud_file","text":"Upload local file cloud storage bucket","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/upload_cloud_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upload a local file to a cloud storage bucket — upload_cloud_file","text":"","code":"upload_cloud_file(file, provider, options, name = file)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/upload_cloud_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upload a local file to a cloud storage bucket — upload_cloud_file","text":"file file-path (character) upload. vector multiple files also supported. provider cloud provider use, either \"gcs\" \"aws\" options named list cloud provider options, see details name call file uploaded. Default filepath","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/upload_cloud_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Upload a local file to a cloud storage bucket — upload_cloud_file","text":"provider \"gcs\" successful list medatada objects","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/upload_cloud_file.html","id":"google-cloud-services","dir":"Reference","previous_headings":"","what":"Google Cloud Services","title":"Upload a local file to a cloud storage bucket — upload_cloud_file","text":"Google Cloud Services (\"gcs\") options must list two fields: bucket bucketname (character) uploading , service_account_key contents authentication json file downloaded Google Project (cloud_storage_authenticate called ). function uses googleCloudStorageR::gcs_upload hood upload file.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/upload_cloud_file.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Upload a local file to a cloud storage bucket — upload_cloud_file","text":"","code":"# Google Cloud Services if (FALSE) { authentication_details <- readLines(\"location_of_json_file.json\") upload_cloud_file(   file = \"table_to_upload.csv\",   provider = \"gcs\",   options = list(     service_account_key = authentication_details,     bucket = \"my-bucket\"   ) ) }"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/upload_dataverse.html","id":null,"dir":"Reference","previous_headings":"","what":"Upload and publish a dataset on Dataverse — upload_dataverse","title":"Upload and publish a dataset on Dataverse — upload_dataverse","text":"function upload publish data specific Dataverse repository including associated metadata information.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/upload_dataverse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upload and publish a dataset on Dataverse — upload_dataverse","text":"","code":"upload_dataverse(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/upload_dataverse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upload and publish a dataset on Dataverse — upload_dataverse","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/upload_files.html","id":null,"dir":"Reference","previous_headings":"","what":"Upload files to Dataverse — upload_files","title":"Upload files to Dataverse — upload_files","text":"function upload list files specific Dataverse repository.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/upload_files.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upload files to Dataverse — upload_files","text":"","code":"upload_files(file_list = NULL, key = NULL, dataverse = NULL, server = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/upload_files.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upload files to Dataverse — upload_files","text":"file_list Paths indicating files uploaded Dataverse. key API token associated Dataverse account. dataverse character string specifying Dataverse ID. server character string specifying Dataverse server.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_catch_params.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate surveys' catch parameters — validate_catch_params","title":"Validate surveys' catch parameters — validate_catch_params","text":"function takes preprocessed landings' matrix uses univariate techniques (see univOutl::LocScaleB) identification outliers distribution number individuals per catch size. function returns data frame survey id, alert number nested column species_group containing validated catches parameters.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_catch_params.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate surveys' catch parameters — validate_catch_params","text":"","code":"validate_catch_params(data = NULL, k_ind = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_catch_params.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate surveys' catch parameters — validate_catch_params","text":"data preprocessed data frame k_ind Extension bounds number individuals","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_catch_params.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate surveys' catch parameters — validate_catch_params","text":"data frame containing validated catches parameters.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_catch_params.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate surveys' catch parameters — validate_catch_params","text":"","code":"if (FALSE) { pars <- read_config() landings <- get_merged_landings(pars) validate_catch_params(landings, k = 3) }"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_catch_price.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate surveys' total catch values — validate_catch_price","title":"Validate surveys' total catch values — validate_catch_price","text":"function takes preprocessed landings' matrix uses univariate techniques (see univOutl::LocScaleB) identification outliers distribution total catch values associated surveys.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_catch_price.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate surveys' total catch values — validate_catch_price","text":"","code":"validate_catch_price(data, method = NULL, k = NULL)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_catch_price.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate surveys' total catch values — validate_catch_price","text":"data preprocessed data frame method character identifying estimate scale distribution. Available choices : method='IQR' using Inter-Quartile Range, .e. Q3-Q1; method='IDR' using Inter-Decile Range; .e. P90-P10 method='MAD' using Median Absolute Deviation; method='Gini' robust scale estimate based Gini's Mean Difference (see  GiniMd); method='ScaleTau2' robust tau-estimate univariate scale, proposed Maronna Zamar (2002) (see alsoscaleTau2); method='Qn' using Qn estimator proposed Rousseeuw Croux (1993) (see also Qn); method='Sn' using Sn estimator proposed Rousseeuw Croux (1993) (see also Sn). method='dQ' estimated scale left tail (Q2-Q1)/0.6745, right tail considered (Q3-Q2)/0.6745 (Q2 median); double estimate able account slight skewness. method='dD' estimated scale left tail (P50-P10)/1.2816, right tail considered (P90-P50)/1.2816 (P50 median); double estimate able account skewness. Finally, method='AdjOut', bounds based adjusted outlyingness method proposed Hubert Van der Veeken (2008). k Nonnegative constant determines extension bounds. Commonly used values 2, 2.5 3 (default).","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_catch_price.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate surveys' total catch values — validate_catch_price","text":"data frame containing validated catch values.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_catch_price.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate surveys' total catch values — validate_catch_price","text":"","code":"if (FALSE) { pars <- read_config() landings <- get_merged_landings(pars) validate_catch_value(landings, method = \"MAD\", k = 13) }"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_landings.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate landings — validate_landings","title":"Validate landings — validate_landings","text":"Downloads preprocessed version data cloud storage services validates range information can safely used analysis. default function uses method median absolute deviation (MAD) outliers identification.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_landings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate landings — validate_landings","text":"","code":"validate_landings(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_landings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate landings — validate_landings","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_landings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate landings — validate_landings","text":"outputs. function used side effects","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_landings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Validate landings — validate_landings","text":"parameters needed config file required preprocess_landings_step_1() preprocess_landings_step_2(), preprocess_metadata_tables() combined, well parameters needed outliers identification hrs, methodandk`.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_pds_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate pds trips duration and distance — validate_pds_data","title":"Validate pds trips duration and distance — validate_pds_data","text":"function takes pds trips data returns validated trip duration distance.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_pds_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate pds trips duration and distance — validate_pds_data","text":"","code":"validate_pds_data(   data,   max_hrs = NULL,   min_hrs = NULL,   km = NULL,   se_km = NULL,   outl = NULL,   timet = NULL )"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_pds_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate pds trips duration and distance — validate_pds_data","text":"data data frame containing pds trips max_hrs Upper limit trip duration (hours) considered valid catch session. min_hrs Lower limit trip duration (hours) considered valid catch session. km Limit trip distance traveled (Km) considered valid catch session. se_km Distance start end point trip considered valid catch session. outl Limit speed outlier points trip considered good quality pds track. timet Limit signal trace dispersion considered good quality pds track.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_pds_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate pds trips duration and distance — validate_pds_data","text":"list containing data frames validated catch duration catch distance traveled","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_pds_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate pds trips duration and distance — validate_pds_data","text":"","code":"if (FALSE) { pars <- read_config() pds_trips <- get_preprocessed_trips() validate_pds(pds_trips) }"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_pds_trips.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate Pelagic Data System trips — validate_pds_trips","title":"Validate Pelagic Data System trips — validate_pds_trips","text":"Downloads preprocessed version pds trips pds tracks disgnostics cloud storage services validates range information can safely used analysis.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_pds_trips.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate Pelagic Data System trips — validate_pds_trips","text":"","code":"validate_pds_trips(log_threshold = logger::DEBUG)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_pds_trips.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate Pelagic Data System trips — validate_pds_trips","text":"log_threshold (standard Apache logj4) log level used threshold logging infrastructure. See logger::log_levels details","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_pds_trips.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate Pelagic Data System trips — validate_pds_trips","text":"outputs. function used side effects","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_pds_trips.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Validate Pelagic Data System trips — validate_pds_trips","text":"parameters needed config file required preprocess_pds_trips(), well parameters needed identify anomalous trips.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_price_weight.html","id":null,"dir":"Reference","previous_headings":"","what":"Outlier identification based on Cook's distance — validate_price_weight","title":"Outlier identification based on Cook's distance — validate_price_weight","text":"function adds additional alert price catch alert dataframes relation price weight assume abnormal values relatively species. relationship weight price mostly linear, function identifies survey IDs Cook's distance higher cook_dist * mean_cook, cook_dist multiplicative coefficient cook_dist average Cook's distance relatively species.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_price_weight.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Outlier identification based on Cook's distance — validate_price_weight","text":"","code":"validate_price_weight(   catch_alerts = NULL,   price_alerts = NULL,   non_regular_ids = NULL,   cook_dist = NULL,   price_weight_min = NULL,   price_weight_max = NULL )"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_price_weight.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Outlier identification based on Cook's distance — validate_price_weight","text":"catch_alerts dataframe catch alerts. price_alerts dataframe price alerts. non_regular_ids dataframe landings regularity alerts. cook_dist number go formula cook_dist * (mean(cooksd)). price_weight_min Min price per weight value threshold. price_weight_max Max price per weight value threshold.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_price_weight.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Outlier identification based on Cook's distance — validate_price_weight","text":"price catch alert' dataframes including outlier identification based Cook's distance.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_price_weight.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Outlier identification based on Cook's distance — validate_price_weight","text":"Currently, cook_dist set 21 default value.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_surveys_time.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate surveys' temporal parameters — validate_surveys_time","title":"Validate surveys' temporal parameters — validate_surveys_time","text":"function takes preprocessed landings' matrix validate temporal info associated survey.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_surveys_time.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate surveys' temporal parameters — validate_surveys_time","text":"","code":"validate_surveys_time(data, hrs = NULL, submission_delay)"},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_surveys_time.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate surveys' temporal parameters — validate_surveys_time","text":"data preprocessed data frame hrs Limit trip duration hours considered valid catch session. submission_delay Limit maximum difference (days) survey submission date recorded landing datw","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_surveys_time.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate surveys' temporal parameters — validate_surveys_time","text":"list containing data frames validated catch dates catch duration.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/reference/validate_surveys_time.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate surveys' temporal parameters — validate_surveys_time","text":"","code":"if (FALSE) { pars <- read_config() landings <- get_merged_landings(pars) validate_surveys_time(landings, hrs = 18) }"},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"improvements-3-1-0","dir":"Changelog","previous_headings":"","what":"Improvements","title":"peskas.timor.data.pipeline 3.1.0","text":"Estimations now weighted based number records monthly bin. improve accuracy estimations reduce outliers effect.","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"new-features-3-0-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.timor.data.pipeline 3.0.0","text":"Integrated ingestion new survey form “Peskas 2”. new form deployed aim manageable enumerators. also includes questions regarding fish handling trading.","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"breaking-changes-2-0-0","dir":"Changelog","previous_headings":"","what":"Breaking changes","title":"peskas.timor.data.pipeline 2.0.0","text":"Validation step now implemented using google sheets using googlesheets4 package.","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"improvements-1-4-0","dir":"Changelog","previous_headings":"","what":"Improvements","title":"peskas.timor.data.pipeline 1.4.0","text":"Improve national municipal estimates combining packags Amelia mice missing outliers data imputation. Integrating price per kg export data","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"bug-fixes-1-4-0","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"peskas.timor.data.pipeline 1.4.0","text":"Improved revenue outliers identification based empirical information","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"new-features-1-3-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.timor.data.pipeline 1.3.0","text":"Integrated taxa selection filtering tracks file deemed leaflet map portal","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"new-features-1-2-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.timor.data.pipeline 1.2.0","text":"Added new feature function ingest_pds_map. function process upload data frame containing number trips, CPE (catch per unit effort) RPE (revenue per unit effort) splitted grids produce leaflet maps web portal.","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"improvements-1-1-0","dir":"Changelog","previous_headings":"","what":"Improvements","title":"peskas.timor.data.pipeline 1.1.0","text":"Improve estimation fish groups’ catches calculating weight per individual case case.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"bug-fixes-1-1-0","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"peskas.timor.data.pipeline 1.1.0","text":"-Fix error cleaning legacy landings: columns indicating number individuals fish length catches > 60cm exchanged","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"improvements-1-0-0","dir":"Changelog","previous_headings":"","what":"Improvements","title":"peskas.timor.data.pipeline 1.0.0","text":"Adding option produce Timor map filtered fishing trips","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"improvements-0-20-0","dir":"Changelog","previous_headings":"","what":"Improvements","title":"peskas.timor.data.pipeline 0.20.0","text":"Improve validation step flagging observation characterized positive revenue (individuals) despite 0 null individuals (revenue). Use log model identify abnormal weight-revenue relationships Cook’s distance estimation. Replace NA catch code catch code “0” ensure observations non positive individuals revenue. Use total length weight calculation MOO landings (weights calculated FL seem quite unrealistic) Use 95° quantile instead median summarise weight parameters catch types, seems return realistic weight estimations single individuals.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"bug-fixes-0-20-0","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"peskas.timor.data.pipeline 0.20.0","text":"Split landings preprocessing two jobs avoid run memory (docker exit code 137)","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"new-features-0-19-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.timor.data.pipeline 0.19.0","text":"Added new function ingest_pds_matched_trips ingest matched pds tracks survey landings zip folder monthly scale.","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"new-features-0-18-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.timor.data.pipeline 0.18.0","text":"Added nutritional values catch function get_nutrients_table. function links repository https://github.com/mamacneil/NutrientFishbase join estimated nutrients values FishBase species data.","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"new-features-0-17-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.timor.data.pipeline 0.17.0","text":"Added folder report containing scripts useful generate pdf report Peskas. report downloadable Peskas portal. Added series functions (get_sync_tracks(), get_full_tracks() ,get_full_trips()) useful retrieve complete file pds tracks. Generate retrieve pds track map ingest_pds_map() get_tracks_map().","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"breaking-changes-0-16-0","dir":"Changelog","previous_headings":"","what":"Breaking changes","title":"peskas.timor.data.pipeline 0.16.0","text":"Docker file now run rocker/r-ver 4.1.1 instead rocker/geospatial:4.0.3","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"bug-fixes-0-16-0","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"peskas.timor.data.pipeline 0.16.0","text":"Fixed bug calling ingest_rfish_table() main pipeline.","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"new-features-0-15-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.timor.data.pipeline 0.15.0","text":"Added series functions (get_catch_types(), get_fish_length() ,retrieve_lengths()) useful retrieve morphometric conversion factors catch types names metadata tables. Added new metadata table airtable (morphometric_table) containing length-weight length-length conversion factors. Added join_weights() integrates morphometric data merged landings.","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"new-features-0-14-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.timor.data.pipeline 0.14.0","text":"test validated data ensure integrity Added merge_trips() integrates data landings tracking together Added format_public_data() format export data used analytics","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"new-features-0-13-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.timor.data.pipeline 0.13.0","text":"Added functions process validate pds trips. Namely, preprocess_pds_trips() validate_pds_trips(). functions make sure data types appropriate, check trip duplicates perform basic checks trip duration distance.","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"new-features-0-11-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.timor.data.pipeline 0.11.0","text":"Added retrieve_pds_trips_data(), retrieve_pds_trips(), retrieve_pds_tracks_data() retrieve_pds_tracks() download trips tracks Pelagic Data System API. Added function get_pds_res() convert Pelagic Data System API responses data frames merge trips tracks data unique file. Added ingest_pds_trips() ingest_pds_tracks() upload Pelagic Data System data cloud. Created new bucket (pds-timor-dev) store pds data.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"bug-fixes-0-11-0","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"peskas.timor.data.pipeline 0.11.0","text":"cloud_object_name() returns empty vector bucket empty","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"bug-fixes-0-10-1","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"peskas.timor.data.pipeline 0.10.1","text":"Fixed bug survey retrieve process. Integrating server response status check retrieve_survey_data()","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"new-features-0-10-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.timor.data.pipeline 0.10.0","text":"Added merge_landings() merge upload pre-processed recent legacy landings data.","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"improvements-0-9-1","dir":"Changelog","previous_headings":"","what":"Improvements","title":"peskas.timor.data.pipeline 0.9.1","text":"Simplified jobs pipeline workflow","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"bug-fixes-0-9-1","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"peskas.timor.data.pipeline 0.9.1","text":"Fixed workflow runs supposed run production actually ","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"new-features-0-9-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.timor.data.pipeline 0.9.0","text":"Added clean_catches(), coalist() clean_legacy_landings() restructure legacy landings recent landings. Added preprocess_legacy_landings() clean ingest preprocessed legacy data.","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"new-features-0-8-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.timor.data.pipeline 0.8.0","text":"Added validate_landings() ingest_validation_tables() get validation data, check ladings upload flags Airtable. Added air_tibble_to_records() air_upload_records() create update records Airtable Added logic validate IMEIs landing data Added script inst/airtable/edit-submission-link.js can used Airtable generate link editable submission form","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"improvements-0-8-0","dir":"Changelog","previous_headings":"New features","what":"Improvements","title":"peskas.timor.data.pipeline 0.8.0","text":"Deactivated check uniqueness IMEI per boat pt_","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"breaking-changes-0-8-0","dir":"Changelog","previous_headings":"New features","what":"Breaking changes","title":"peskas.timor.data.pipeline 0.8.0","text":"Landings pre-processing now renames columns data: specifically submission_id (used *_id) landing_date* (used date)","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"new-features-0-7-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.timor.data.pipeline 0.7.0","text":"Added ingest_legacy_landings retrieve data legacy data (SFF landings)","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"new-features-0-6-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.timor.data.pipeline 0.6.0","text":"Added air_get_records() air_records_to_tibble() retrieve process records Airtable Added pt_validate_boats(), pt_validate_devices(), pt_validate_vms_installs() perform basic data validation metadata tables","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"improvements-0-6-0","dir":"Changelog","previous_headings":"","what":"Improvements","title":"peskas.timor.data.pipeline 0.6.0","text":"cloud_object_name() can now also match files exact name just prefix","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"breaking-changes-0-6-0","dir":"Changelog","previous_headings":"","what":"Breaking changes","title":"peskas.timor.data.pipeline 0.6.0","text":"ingest_metadata_tables() preprocess_metadata_tables() now use logic use Airtable instead og Google Sheets","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"new-features-0-5-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.timor.data.pipeline 0.5.0","text":"Added ingest_metadata_tables() ingest data boats, species, municipalities, etc. Added preprocess_metadata_tables() preprocess data metadata ingestion. Added pt_get_devices_table() pt_validate_flags() helper functions metadata preprocessing.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"improvements-0-5-0","dir":"Changelog","previous_headings":"","what":"Improvements","title":"peskas.timor.data.pipeline 0.5.0","text":"use single function read config.file load environment parameters Added skeleton pipeline provide guidance future development. now visible README.","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"breaking-changes-0-4-0","dir":"Changelog","previous_headings":"","what":"Breaking changes","title":"peskas.timor.data.pipeline 0.4.0","text":"Renamed ingest_timor_landings() ingest_landings() brevity functions relate Timor anyways.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"improvements-0-4-0","dir":"Changelog","previous_headings":"","what":"Improvements","title":"peskas.timor.data.pipeline 0.4.0","text":"Renamed job names github workflow functions job calls. Improved documentation package: particularly readme function reference.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"bug-fixes-0-4-0","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"peskas.timor.data.pipeline 0.4.0","text":"now skip RCurl windows tests","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"new-features-0-3-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.timor.data.pipeline 0.3.0","text":"preprocessing East Timor landings implemented preprocess_landings() Added pt_nest_attachments() group attachment columns nested column containing data frames. Added pt_nest_species() group attachment columns nested column containing data frames. Added cloud_object_name() complement add_version() return latest specified version object storage location. Added download_cloud_file() download files cloud storage providers.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"improvements-0-3-0","dir":"Changelog","previous_headings":"","what":"Improvements","title":"peskas.timor.data.pipeline 0.3.0","text":"Now using cloud_storage_authenticate() internally authenticate cloud storage instead authenticating separately cloud functionjj. simplifies authentication ensures authentication attempted credentials already validated.","code":""},{"path":[]},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"breaking-changes-0-2-0","dir":"Changelog","previous_headings":"","what":"Breaking changes","title":"peskas.timor.data.pipeline 0.2.0","text":"download_survey_data(), download_survey_metadata(), download_survey() renamed retrieve_survey_data(), retrieve_survey_metadata(), retrieve_survey(). avoid confusion planned functions download data cloud locations. suffix raw metadata appended prefix retrieving survey information now separated using \"_\" rather “-”. easily distinguish information encoded file name.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"new-features-0-2-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.timor.data.pipeline 0.2.0","text":"prefix name surveys hard-coded can specified config file (file_prefix field).","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"peskastimordatapipeline-010","dir":"Changelog","previous_headings":"","what":"peskas.timor.data.pipeline 0.1.0","title":"peskas.timor.data.pipeline 0.1.0","text":"Adds infrastructure download survey data upload cloud storage providers implements ingestion East Timor landings.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"new-features-0-1-0","dir":"Changelog","previous_headings":"","what":"New features","title":"peskas.timor.data.pipeline 0.1.0","text":"ingestion East Timor Landings implemented ingest_timor_landings(). functions download_survey_data() download_survey_metadata() download data metadata electronic survey hosted kobo, kobohr, ona. download_survey() can used wrapper download data metadata single call. upload_cloud_file() can used upload set files cloud storage bucket. Currently Google Cloud Services (GCS) supported. add_version() utility function appends date-time sha information string used version file names. get_host_url() utility function gets host url electronic survey provider API.","code":""},{"path":"https://worldfishcenter.github.io/peskas.timor.data.pipeline/news/index.html","id":"pipeline-0-1-0","dir":"Changelog","previous_headings":"","what":"Pipeline","title":"peskas.timor.data.pipeline 0.1.0","text":"data pipeline implemented run GitHub Actions schedule.","code":""}]
